{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Handwriting Image Classifier (PyTorch)\n",
        "\n",
        "This notebook trains a classifier for handwriting images using the datasets in `Assets/`:\n",
        "- `augmented_images/`: directory-of-directories (each subfolder is a class)\n",
        "- `handwritten-english-characters-and-digits/`: has `train/` and `test/` splits (each has class subfolders)\n",
        "- `image_labels.csv`: has `filename,label` mapping; we'll search for the files within `Assets/`\n",
        "\n",
        "Flow:\n",
        "- Discover and merge samples from all three sources\n",
        "- Build stratified train/val/test splits\n",
        "- Train a pretrained ResNet18-based model (with a CPU-friendly v2-fast option)\n",
        "- Evaluate and save the best model and label mapping\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in c:\\users\\royha\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.3.1)\n",
            "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\royha\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2.2.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\royha\\appdata\\roaming\\python\\python313\\site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\royha\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\royha\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\royha\\appdata\\roaming\\python\\python313\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.0.1 -> 25.2\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "pip install pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU threads set to: 4\n"
          ]
        }
      ],
      "source": [
        "# System/perf settings\n",
        "import multiprocessing as mp\n",
        "from PIL import ImageFile\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "# Environment & imports\n",
        "import os, sys, json, random, math, time\n",
        "from pathlib import Path\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "# Cap CPU threads to avoid oversubscription\n",
        "try:\n",
        "    torch.set_num_threads(max(1, min(os.cpu_count() or 4, 8)))\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "print('CPU threads set to:', torch.get_num_threads())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "try:\n",
        "    import torchvision\n",
        "    from torchvision import transforms\n",
        "except Exception as e:\n",
        "    print(\"torchvision not found; attempting to continue with PIL-only transforms\")\n",
        "    torchvision = None\n",
        "    transforms = None\n",
        "\n",
        "ASSETS = Path('Assets')\n",
        "assert ASSETS.exists(), f\"Assets folder not found at {ASSETS.resolve()}\"\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "augmented_images: 13640 samples\n",
            "HED train: 2728 samples, HED test: 682 samples\n",
            "CSV samples resolved from image_labels.csv: 0 (missing: 13640)\n",
            "Total unique samples: 17050\n",
            "Num classes: 62\n",
            "Class distribution (top 20): [('0', 275), ('1', 275), ('2', 275), ('3', 275), ('4', 275), ('5', 275), ('6', 275), ('7', 275), ('8', 275), ('9', 275), ('a', 275), ('A_caps', 275), ('b', 275), ('B_caps', 275), ('c', 275), ('C_caps', 275), ('d', 275), ('D_caps', 275), ('e', 275), ('E_caps', 275)]\n"
          ]
        }
      ],
      "source": [
        "# Discover datasets\n",
        "from typing import List, Tuple\n",
        "\n",
        "IMG_EXTS = {'.png', '.jpg', '.jpeg', '.bmp', '.gif'}\n",
        "\n",
        "def list_images_in_dir_of_dirs(root: Path) -> List[Tuple[Path, str]]:\n",
        "    samples = []\n",
        "    if not root.exists():\n",
        "        return samples\n",
        "    for class_dir in sorted([p for p in root.iterdir() if p.is_dir()]):\n",
        "        label = class_dir.name\n",
        "        for p in class_dir.rglob('*'):\n",
        "            if p.suffix.lower() in IMG_EXTS and p.is_file():\n",
        "                samples.append((p, label))\n",
        "    return samples\n",
        "\n",
        "# 1) augmented_images\n",
        "aug_dir = ASSETS / 'augmented_images'\n",
        "aug_samples = list_images_in_dir_of_dirs(aug_dir)\n",
        "print(f\"augmented_images: {len(aug_samples)} samples\")\n",
        "\n",
        "# 2) handwritten-english-characters-and-digits/train and test\n",
        "hed_root = ASSETS / 'handwritten-english-characters-and-digits'\n",
        "hed_train = list_images_in_dir_of_dirs(hed_root / 'train')\n",
        "hed_test = list_images_in_dir_of_dirs(hed_root / 'test')\n",
        "print(f\"HED train: {len(hed_train)} samples, HED test: {len(hed_test)} samples\")\n",
        "\n",
        "# 3) image_labels.csv: filename,label\n",
        "csv_path = ASSETS / 'image_labels.csv'\n",
        "if not csv_path.exists():\n",
        "    alt_csv_path = ASSETS / 'image_label.csv'  # support alternate name\n",
        "    if alt_csv_path.exists():\n",
        "        csv_path = alt_csv_path\n",
        "\n",
        "csv_samples = []\n",
        "if csv_path.exists():\n",
        "    df = pd.read_csv(csv_path)\n",
        "    assert {'filename','label'}.issubset(df.columns)\n",
        "    # Index all images under Assets for filename lookup\n",
        "    all_imgs = {p.name: p for p in ASSETS.rglob('*') if p.suffix.lower() in IMG_EXTS}\n",
        "    missing = 0\n",
        "    for _, row in df.iterrows():\n",
        "        fname = str(row['filename'])\n",
        "        label = str(row['label'])\n",
        "        if fname in all_imgs:\n",
        "            csv_samples.append((all_imgs[fname], label))\n",
        "        else:\n",
        "            # Try to search by suffix match if duplicates are unlikely\n",
        "            matches = [p for n,p in all_imgs.items() if n.endswith(fname)]\n",
        "            if len(matches) == 1:\n",
        "                csv_samples.append((matches[0], label))\n",
        "            else:\n",
        "                missing += 1\n",
        "    print(f\"CSV samples resolved from {csv_path.name}: {len(csv_samples)} (missing: {missing})\")\n",
        "else:\n",
        "    print(\"image_labels.csv not found; skipping CSV source\")\n",
        "\n",
        "# Merge all; if duplicates appear, prefer explicit CSV labels > hed > aug\n",
        "# Use image absolute path as key\n",
        "merged = {}\n",
        "for p,l in aug_samples:\n",
        "    merged[str(p.resolve())] = (p, l)\n",
        "for p,l in hed_train + hed_test:\n",
        "    merged[str(p.resolve())] = (p, l)\n",
        "for p,l in csv_samples:\n",
        "    merged[str(p.resolve())] = (p, l)\n",
        "\n",
        "all_samples = list(merged.values())\n",
        "print(f\"Total unique samples: {len(all_samples)}\")\n",
        "\n",
        "# Map labels to indices\n",
        "labels = sorted(sorted({l for _, l in all_samples}))\n",
        "label_to_idx = {l:i for i,l in enumerate(labels)}\n",
        "idx_to_label = {i:l for l,i in label_to_idx.items()}\n",
        "print(f\"Num classes: {len(labels)}\")\n",
        "\n",
        "# Class distribution\n",
        "counts = Counter([l for _,l in all_samples])\n",
        "print(\"Class distribution (top 20):\", counts.most_common(20))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "13640 1705 1705\n"
          ]
        }
      ],
      "source": [
        "# Train/Val/Test split (stratified)\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "random_seed = 42\n",
        "rng = np.random.RandomState(random_seed)\n",
        "\n",
        "paths = np.array([str(p) for p,_ in all_samples])\n",
        "labels_arr = np.array([label_to_idx[l] for _,l in all_samples])\n",
        "\n",
        "# If HED has an explicit test set, we already included it. We'll still split overall\n",
        "# into train/val/test=0.8/0.1/0.1 stratified.\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(\n",
        "    paths, labels_arr, test_size=0.1, random_state=random_seed, stratify=labels_arr\n",
        ")\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_temp, y_temp, test_size=0.1111, random_state=random_seed, stratify=y_temp\n",
        ")  # 0.8 * 0.1111 ≈ 0.0889 so final ≈ 80/10/10\n",
        "\n",
        "print(len(X_train), len(X_val), len(X_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# (Removed) Baseline transforms and dataset — superseded by v2. Keeping small helper for compatibility.\n",
        "IMG_SIZE = 64\n",
        "basic_train_tfms = None\n",
        "basic_val_tfms = None\n",
        "\n",
        "class HandwritingDataset(Dataset):\n",
        "    def __init__(self, paths: np.ndarray, labels: np.ndarray, transform=None):\n",
        "        self.paths = paths\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "    def __len__(self):\n",
        "        return len(self.paths)\n",
        "    def __getitem__(self, idx):\n",
        "        p = Path(self.paths[idx])\n",
        "        y = int(self.labels[idx])\n",
        "        with Image.open(p) as img:\n",
        "            img = img.convert('L')\n",
        "            if self.transform is not None:\n",
        "                x = self.transform(img)\n",
        "            else:\n",
        "                # minimal fallback\n",
        "                img = img.resize((IMG_SIZE, IMG_SIZE))\n",
        "                arr = np.array(img, dtype=np.float32)/255.0\n",
        "                arr = arr[None, ...]\n",
        "                x = torch.from_numpy(arr)\n",
        "        return x, y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(13640, 1705, 1705)"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# (Removed) Baseline dataloaders — use v2 or v2-fast loaders below\n",
        "'Use cells 17–19 for training/eval loaders.'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(2206462,\n",
              " SmallCNN(\n",
              "   (features): Sequential(\n",
              "     (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "     (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "     (2): ReLU(inplace=True)\n",
              "     (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "     (4): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "     (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "     (6): ReLU(inplace=True)\n",
              "     (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "     (8): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "     (9): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "     (10): ReLU(inplace=True)\n",
              "     (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "   )\n",
              "   (classifier): Sequential(\n",
              "     (0): Dropout(p=0.3, inplace=False)\n",
              "     (1): Linear(in_features=8192, out_features=256, bias=True)\n",
              "     (2): ReLU(inplace=True)\n",
              "     (3): Dropout(p=0.3, inplace=False)\n",
              "     (4): Linear(in_features=256, out_features=62, bias=True)\n",
              "   )\n",
              " ))"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# (Removed) SmallCNN baseline — using pretrained ResNet18 (v2)\n",
        "'Use cells 11–19 for model, training and evaluation.'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\royha\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 01/15 - loss: 4.1028 - val_acc: 0.0575\n",
            "Saved new best model to artifacts\\handwriting_cnn.pt (val_acc=0.0575)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\royha\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 02/15 - loss: 3.6793 - val_acc: 0.1959\n",
            "Saved new best model to artifacts\\handwriting_cnn.pt (val_acc=0.1959)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\royha\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 03/15 - loss: 3.0769 - val_acc: 0.3619\n",
            "Saved new best model to artifacts\\handwriting_cnn.pt (val_acc=0.3619)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\royha\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 04/15 - loss: 2.7654 - val_acc: 0.4094\n",
            "Saved new best model to artifacts\\handwriting_cnn.pt (val_acc=0.4094)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\royha\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 05/15 - loss: 2.5328 - val_acc: 0.4522\n",
            "Saved new best model to artifacts\\handwriting_cnn.pt (val_acc=0.4522)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\royha\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 06/15 - loss: 2.3462 - val_acc: 0.5196\n",
            "Saved new best model to artifacts\\handwriting_cnn.pt (val_acc=0.5196)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\royha\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 07/15 - loss: 2.2319 - val_acc: 0.5724\n",
            "Saved new best model to artifacts\\handwriting_cnn.pt (val_acc=0.5724)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\royha\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 08/15 - loss: 2.1515 - val_acc: 0.5736\n",
            "Saved new best model to artifacts\\handwriting_cnn.pt (val_acc=0.5736)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\royha\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 09/15 - loss: 2.0736 - val_acc: 0.6240\n",
            "Saved new best model to artifacts\\handwriting_cnn.pt (val_acc=0.6240)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\royha\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10/15 - loss: 2.0110 - val_acc: 0.6416\n",
            "Saved new best model to artifacts\\handwriting_cnn.pt (val_acc=0.6416)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\royha\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 11/15 - loss: 1.9877 - val_acc: 0.5789\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\royha\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 12/15 - loss: 1.9472 - val_acc: 0.5320\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\royha\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 42\u001b[39m\n\u001b[32m     40\u001b[39m logits = model(xb)\n\u001b[32m     41\u001b[39m loss = criterion(logits, yb)\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m optimizer.step()\n\u001b[32m     44\u001b[39m running_loss += loss.item()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\royha\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\royha\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\autograd\\__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\royha\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\autograd\\graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "# (Removed) Baseline training utilities — replaced by v2 training below\n",
        "'Use cells 18–19 (or 15–16) for training and evaluation.'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy: 0.0416\n",
            "Classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        27\n",
            "           1       0.00      0.00      0.00        28\n",
            "           2       0.00      0.00      0.00        28\n",
            "           3       0.00      0.00      0.00        27\n",
            "           4       0.00      0.00      0.00        27\n",
            "           5       0.00      0.00      0.00        27\n",
            "           6       0.00      0.00      0.00        27\n",
            "           7       0.00      0.00      0.00        28\n",
            "           8       0.03      1.00      0.05        27\n",
            "           9       0.00      0.00      0.00        27\n",
            "      A_caps       0.00      0.00      0.00        28\n",
            "      B_caps       0.00      0.00      0.00        28\n",
            "      C_caps       0.00      0.00      0.00        27\n",
            "      D_caps       0.00      0.00      0.00        27\n",
            "      E_caps       0.00      0.00      0.00        28\n",
            "      F_caps       0.00      0.00      0.00        27\n",
            "      G_caps       0.00      0.00      0.00        27\n",
            "      H_caps       0.00      0.00      0.00        27\n",
            "      I_caps       0.00      0.00      0.00        28\n",
            "      J_caps       0.00      0.00      0.00        27\n",
            "      K_caps       0.00      0.00      0.00        28\n",
            "      L_caps       0.00      0.00      0.00        28\n",
            "      M_caps       0.00      0.00      0.00        28\n",
            "      N_caps       0.09      0.07      0.08        28\n",
            "      O_caps       0.00      0.00      0.00        28\n",
            "      P_caps       0.00      0.00      0.00        27\n",
            "      Q_caps       0.00      0.00      0.00        28\n",
            "      R_caps       0.00      0.00      0.00        28\n",
            "      S_caps       0.00      0.00      0.00        28\n",
            "      T_caps       0.00      0.00      0.00        28\n",
            "      U_caps       0.00      0.00      0.00        27\n",
            "      V_caps       0.08      0.96      0.14        27\n",
            "      W_caps       0.05      0.39      0.08        28\n",
            "      X_caps       0.07      0.19      0.10        27\n",
            "      Y_caps       0.00      0.00      0.00        27\n",
            "      Z_caps       0.00      0.00      0.00        28\n",
            "           a       0.00      0.00      0.00        27\n",
            "           b       0.00      0.00      0.00        28\n",
            "           c       0.00      0.00      0.00        28\n",
            "           d       0.00      0.00      0.00        28\n",
            "           e       0.00      0.00      0.00        28\n",
            "           f       0.00      0.00      0.00        27\n",
            "           g       0.00      0.00      0.00        28\n",
            "           h       0.00      0.00      0.00        28\n",
            "           i       0.00      0.00      0.00        28\n",
            "           j       0.00      0.00      0.00        27\n",
            "           k       0.00      0.00      0.00        28\n",
            "           l       0.00      0.00      0.00        27\n",
            "           m       0.00      0.00      0.00        27\n",
            "           n       0.00      0.00      0.00        28\n",
            "           o       0.00      0.00      0.00        27\n",
            "           p       0.00      0.00      0.00        28\n",
            "           q       0.00      0.00      0.00        27\n",
            "           r       0.00      0.00      0.00        27\n",
            "           s       0.00      0.00      0.00        27\n",
            "           t       0.00      0.00      0.00        28\n",
            "           u       0.00      0.00      0.00        27\n",
            "           v       0.00      0.00      0.00        27\n",
            "           w       0.00      0.00      0.00        27\n",
            "           x       0.00      0.00      0.00        27\n",
            "           y       0.00      0.00      0.00        28\n",
            "           z       0.00      0.00      0.00        28\n",
            "\n",
            "    accuracy                           0.04      1705\n",
            "   macro avg       0.01      0.04      0.01      1705\n",
            "weighted avg       0.01      0.04      0.01      1705\n",
            "\n",
            "Confusion matrix shape: (62, 62)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\royha\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
            "c:\\Users\\royha\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
            "c:\\Users\\royha\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved predictions to artifacts\\test_predictions.csv\n"
          ]
        }
      ],
      "source": [
        "# (Removed) Baseline evaluation — use v2/v2-fast evaluation cells (16 or 19)\n",
        "'Use the v2 model: artifacts_v2/handwriting_resnet18_best.pt'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Improved configuration (v2)\n",
        "IMG_SIZE_V2 = 128\n",
        "CHANNELS_V2 = 3  # use 3-channel to leverage pretrained backbones\n",
        "EPOCHS_V2 = 30\n",
        "BATCH_SIZE_V2 = 128\n",
        "WEIGHT_DECAY_V2 = 1e-4\n",
        "LABEL_SMOOTHING_V2 = 0.1\n",
        "RANDOM_SEED_V2 = 42\n",
        "USE_PRETRAINED_V2 = True  # will fallback automatically if not available\n",
        "\n",
        "rng_v2 = np.random.RandomState(RANDOM_SEED_V2)\n",
        "\n",
        "# Create artifacts dir for v2\n",
        "save_dir_v2 = Path('artifacts_v2')\n",
        "save_dir_v2.mkdir(exist_ok=True)\n",
        "model_path_v2 = save_dir_v2 / 'handwriting_resnet18_best.pt'\n",
        "labels_path_v2 = save_dir_v2 / 'labels.json'\n",
        "with open(labels_path_v2, 'w') as f:\n",
        "    json.dump(idx_to_label, f, indent=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to C:\\Users\\royha/.cache\\torch\\hub\\checkpoints\\resnet18-f37072fd.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100.0%\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using ResNet18 backbone for v2. Imagenet norm: True\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(11208318, 'ResNet')"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Model v2: ResNet18 backbone with pretrained fallback\n",
        "from typing import Optional\n",
        "\n",
        "USE_IMAGENET_NORM_V2 = False\n",
        "\n",
        "def build_resnet18_head(num_classes: int, pretrained: bool = True) -> nn.Module:\n",
        "    global USE_IMAGENET_NORM_V2\n",
        "    if torchvision is None:\n",
        "        raise RuntimeError('torchvision not available')\n",
        "    try:\n",
        "        try:\n",
        "            # torchvision >=0.13\n",
        "            from torchvision.models import resnet18, ResNet18_Weights\n",
        "            weights = ResNet18_Weights.IMAGENET1K_V1 if pretrained else None\n",
        "            model = resnet18(weights=weights)\n",
        "            USE_IMAGENET_NORM_V2 = pretrained\n",
        "        except Exception:\n",
        "            # older API\n",
        "            from torchvision.models import resnet18\n",
        "            model = resnet18(pretrained=pretrained)\n",
        "            USE_IMAGENET_NORM_V2 = pretrained\n",
        "    except Exception as e:\n",
        "        print('Pretrained resnet18 unavailable, falling back to randomly initialized.')\n",
        "        from torchvision.models import resnet18\n",
        "        model = resnet18(pretrained=False)\n",
        "        USE_IMAGENET_NORM_V2 = False\n",
        "    # Adjust first layer to accept 3-channel grayscale (we feed 3-channel grayscale, so no change needed)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, num_classes)\n",
        "    return model\n",
        "\n",
        "try:\n",
        "    model_v2 = build_resnet18_head(len(labels), pretrained=USE_PRETRAINED_V2).to(device)\n",
        "    print('Using ResNet18 backbone for v2. Imagenet norm:', USE_IMAGENET_NORM_V2)\n",
        "except Exception as e:\n",
        "    print('Falling back to SmallCNN due to error:', e)\n",
        "    CHANNELS_V2 = 1\n",
        "    class DeeperCNN(nn.Module):\n",
        "        def __init__(self, num_classes: int):\n",
        "            super().__init__()\n",
        "            self.features = nn.Sequential(\n",
        "                nn.Conv2d(1, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(),\n",
        "                nn.Conv2d(64, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(),\n",
        "                nn.MaxPool2d(2),\n",
        "                nn.Conv2d(64, 128, 3, padding=1), nn.BatchNorm2d(128), nn.ReLU(),\n",
        "                nn.Conv2d(128, 128, 3, padding=1), nn.BatchNorm2d(128), nn.ReLU(),\n",
        "                nn.MaxPool2d(2),\n",
        "                nn.Conv2d(128, 256, 3, padding=1), nn.BatchNorm2d(256), nn.ReLU(),\n",
        "                nn.MaxPool2d(2),\n",
        "            )\n",
        "            fsize = IMG_SIZE_V2 // 8\n",
        "            self.classifier = nn.Sequential(\n",
        "                nn.Dropout(0.4), nn.Linear(256*fsize*fsize, 512), nn.ReLU(),\n",
        "                nn.Dropout(0.4), nn.Linear(512, num_classes)\n",
        "            )\n",
        "        def forward(self, x):\n",
        "            x = self.features(x)\n",
        "            x = torch.flatten(x, 1)\n",
        "            return self.classifier(x)\n",
        "    model_v2 = DeeperCNN(len(labels)).to(device)\n",
        "    USE_IMAGENET_NORM_V2 = False\n",
        "\n",
        "sum(p.numel() for p in model_v2.parameters() if p.requires_grad), model_v2.__class__.__name__\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Transforms v2 (with optional dataset normalization)\n",
        "from typing import Tuple\n",
        "\n",
        "def compute_mean_std(paths_subset: np.ndarray, channels: int, sample_size: int = 1024) -> Tuple[list, list]:\n",
        "    sample_paths = paths_subset.copy()\n",
        "    if len(sample_paths) > sample_size:\n",
        "        sample_paths = rng_v2.choice(sample_paths, size=sample_size, replace=False)\n",
        "    means = np.zeros(channels, dtype=np.float64)\n",
        "    sq_means = np.zeros(channels, dtype=np.float64)\n",
        "    n_pix_total = 0\n",
        "    for p in sample_paths:\n",
        "        with Image.open(p) as img:\n",
        "            img = img.convert('L')  # grayscale base\n",
        "            img = img.resize((IMG_SIZE_V2, IMG_SIZE_V2))\n",
        "            arr = np.array(img, dtype=np.float32) / 255.0\n",
        "            if channels == 1:\n",
        "                arr_c = arr[None, ...]\n",
        "            else:\n",
        "                arr_c = np.stack([arr, arr, arr], axis=0)\n",
        "            n_pix = arr_c.shape[1] * arr_c.shape[2]\n",
        "            means += arr_c.reshape(channels, -1).sum(axis=1)\n",
        "            sq_means += (arr_c.reshape(channels, -1) ** 2).sum(axis=1)\n",
        "            n_pix_total += n_pix\n",
        "    means /= n_pix_total\n",
        "    stds = np.sqrt(sq_means / n_pix_total - means**2)\n",
        "    return means.tolist(), stds.tolist()\n",
        "\n",
        "if transforms is None:\n",
        "    raise RuntimeError('torchvision is required for v2 pipeline transforms')\n",
        "\n",
        "if USE_IMAGENET_NORM_V2:\n",
        "    # ImageNet mean/std\n",
        "    mean_v2 = [0.485, 0.456, 0.406]\n",
        "    std_v2 = [0.229, 0.224, 0.225]\n",
        "else:\n",
        "    mean_v2, std_v2 = compute_mean_std(X_train, CHANNELS_V2, sample_size=1024)\n",
        "\n",
        "train_tfms_v2 = transforms.Compose([\n",
        "    transforms.Grayscale(num_output_channels=CHANNELS_V2),\n",
        "    transforms.Resize(int(IMG_SIZE_V2*1.1)),\n",
        "    transforms.RandomCrop(IMG_SIZE_V2),\n",
        "    transforms.RandomRotation(8, fill=0),\n",
        "    transforms.RandomAffine(degrees=0, shear=8, translate=(0.05,0.05)),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
        "    transforms.RandomPerspective(distortion_scale=0.2, p=0.3),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean_v2, std_v2),\n",
        "])\n",
        "\n",
        "val_tfms_v2 = transforms.Compose([\n",
        "    transforms.Grayscale(num_output_channels=CHANNELS_V2),\n",
        "    transforms.Resize((IMG_SIZE_V2, IMG_SIZE_V2)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean_v2, std_v2),\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(13640,\n",
              " 1705,\n",
              " 1705,\n",
              " [(34, 220),\n",
              "  (42, 220),\n",
              "  (52, 220),\n",
              "  (53, 220),\n",
              "  (37, 220),\n",
              "  (47, 220),\n",
              "  (50, 220),\n",
              "  (17, 220),\n",
              "  (55, 220),\n",
              "  (51, 220)])"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "# Dataset & DataLoaders v2 with class balancing\n",
        "from torch.utils.data import WeightedRandomSampler\n",
        "\n",
        "class HandwritingDatasetV2(Dataset):\n",
        "    def __init__(self, paths: np.ndarray, labels: np.ndarray, transform=None):\n",
        "        self.paths = paths\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "    def __len__(self):\n",
        "        return len(self.paths)\n",
        "    def __getitem__(self, idx):\n",
        "        p = Path(self.paths[idx])\n",
        "        y = int(self.labels[idx])\n",
        "        with Image.open(p) as img:\n",
        "            img = img.convert('L')\n",
        "            x = (self.transform or val_tfms_v2)(img)\n",
        "        return x, y\n",
        "\n",
        "train_ds_v2 = HandwritingDatasetV2(X_train, y_train, transform=train_tfms_v2)\n",
        "val_ds_v2   = HandwritingDatasetV2(X_val, y_val, transform=val_tfms_v2)\n",
        "test_ds_v2  = HandwritingDatasetV2(X_test, y_test, transform=val_tfms_v2)\n",
        "\n",
        "# Class weights for sampler and loss\n",
        "class_counts = Counter(y_train.tolist())\n",
        "num_classes = len(labels)\n",
        "class_freq = np.array([class_counts.get(i, 0) for i in range(num_classes)], dtype=np.float64)\n",
        "class_weights = 1.0 / np.maximum(class_freq, 1.0)\n",
        "class_weights = class_weights / class_weights.sum() * num_classes\n",
        "\n",
        "# Sample weight per instance\n",
        "sample_weights = np.array([class_weights[y] for y in y_train], dtype=np.float64)\n",
        "sampler = WeightedRandomSampler(weights=torch.DoubleTensor(sample_weights), num_samples=len(sample_weights), replacement=True)\n",
        "\n",
        "num_workers_v2 = 0 if os.name == 'nt' else 2\n",
        "train_loader_v2 = DataLoader(train_ds_v2, batch_size=BATCH_SIZE_V2, sampler=sampler, num_workers=num_workers_v2, pin_memory=True)\n",
        "val_loader_v2   = DataLoader(val_ds_v2, batch_size=BATCH_SIZE_V2, shuffle=False, num_workers=num_workers_v2)\n",
        "test_loader_v2  = DataLoader(test_ds_v2, batch_size=BATCH_SIZE_V2, shuffle=False, num_workers=num_workers_v2)\n",
        "\n",
        "len(train_ds_v2), len(val_ds_v2), len(test_ds_v2), class_counts.most_common(10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\royha\\AppData\\Local\\Temp\\ipykernel_13936\\2646473394.py:12: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler(enabled=(device.type=='cuda'))\n",
            "c:\\Users\\royha\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "C:\\Users\\royha\\AppData\\Local\\Temp\\ipykernel_13936\\2646473394.py:28: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=(device.type=='cuda')):\n",
            "C:\\Users\\royha\\AppData\\Local\\Temp\\ipykernel_13936\\2646473394.py:46: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[v2] Epoch 01/30 - loss: 1.8219 - val_acc: 0.8117 - val_macro_f1: 0.8068\n",
            "Saved best v2 model (val_macro_f1=0.8068) -> artifacts_v2\\handwriting_resnet18_best.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\royha\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "C:\\Users\\royha\\AppData\\Local\\Temp\\ipykernel_13936\\2646473394.py:28: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=(device.type=='cuda')):\n"
          ]
        }
      ],
      "source": [
        "# Training loop v2 with AMP, label smoothing, AdamW, OneCycleLR, early stopping on macro F1\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "criterion_v2 = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32, device=device),\n",
        "                                   label_smoothing=LABEL_SMOOTHING_V2)\n",
        "optimizer_v2 = torch.optim.AdamW(model_v2.parameters(), lr=3e-3, weight_decay=WEIGHT_DECAY_V2)\n",
        "steps_per_epoch = max(1, math.ceil(len(train_ds_v2) / BATCH_SIZE_V2))\n",
        "scheduler_v2 = torch.optim.lr_scheduler.OneCycleLR(optimizer_v2, max_lr=3e-3, steps_per_epoch=steps_per_epoch,\n",
        "                                                   epochs=EPOCHS_V2, pct_start=0.2, div_factor=10.0, final_div_factor=10.0)\n",
        "\n",
        "scaler = GradScaler(enabled=(device.type=='cuda'))\n",
        "\n",
        "best_val_f1 = 0.0\n",
        "patience = 6\n",
        "pat = 0\n",
        "\n",
        "history = {'epoch': [], 'train_loss': [], 'val_acc': [], 'val_f1': []}\n",
        "\n",
        "for epoch in range(1, EPOCHS_V2+1):\n",
        "    model_v2.train()\n",
        "    running_loss = 0.0\n",
        "    b = 0\n",
        "    for xb, yb in train_loader_v2:\n",
        "        xb = xb.to(device, non_blocking=True)\n",
        "        yb = yb.to(device, non_blocking=True)\n",
        "        optimizer_v2.zero_grad(set_to_none=True)\n",
        "        with autocast(enabled=(device.type=='cuda')):\n",
        "            logits = model_v2(xb)\n",
        "            loss = criterion_v2(logits, yb)\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer_v2)\n",
        "        scaler.update()\n",
        "        scheduler_v2.step()\n",
        "        running_loss += loss.item()\n",
        "        b += 1\n",
        "    train_loss = running_loss / max(1, b)\n",
        "\n",
        "    # Validate\n",
        "    model_v2.eval()\n",
        "    ys, ys_pred = [], []\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in val_loader_v2:\n",
        "            xb = xb.to(device)\n",
        "            yb = yb.to(device)\n",
        "            with autocast(enabled=(device.type=='cuda')):\n",
        "                logits = model_v2(xb)\n",
        "            preds = logits.argmax(dim=1)\n",
        "            ys.extend(yb.cpu().numpy().tolist())\n",
        "            ys_pred.extend(preds.cpu().numpy().tolist())\n",
        "    val_acc = accuracy_score(ys, ys_pred)\n",
        "    val_f1 = f1_score(ys, ys_pred, average='macro')\n",
        "    history['epoch'].append(epoch)\n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['val_acc'].append(val_acc)\n",
        "    history['val_f1'].append(val_f1)\n",
        "\n",
        "    print(f\"[v2] Epoch {epoch:02d}/{EPOCHS_V2} - loss: {train_loss:.4f} - val_acc: {val_acc:.4f} - val_macro_f1: {val_f1:.4f}\")\n",
        "\n",
        "    if val_f1 > best_val_f1:\n",
        "        best_val_f1 = val_f1\n",
        "        torch.save({'model_state': model_v2.state_dict(),\n",
        "                    'config': {'img_size': IMG_SIZE_V2, 'num_classes': len(labels), 'channels': CHANNELS_V2,\n",
        "                               'mean': mean_v2, 'std': std_v2, 'backbone': model_v2.__class__.__name__}},\n",
        "                   model_path_v2)\n",
        "        print(f\"Saved best v2 model (val_macro_f1={best_val_f1:.4f}) -> {model_path_v2}\")\n",
        "        pat = 0\n",
        "    else:\n",
        "        pat += 1\n",
        "        if pat >= patience:\n",
        "            print('Early stopping triggered')\n",
        "            break\n",
        "\n",
        "json.dump(history, open(save_dir_v2 / 'train_history.json', 'w'), indent=2)\n",
        "print('Best v2 val macro F1:', best_val_f1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test evaluation v2\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "if model_path_v2.exists():\n",
        "    ckpt = torch.load(model_path_v2, map_location=device)\n",
        "    model_v2.load_state_dict(ckpt['model_state'])\n",
        "\n",
        "model_v2.eval()\n",
        "ys, ys_pred = [], []\n",
        "with torch.no_grad():\n",
        "    for xb, yb in test_loader_v2:\n",
        "        xb = xb.to(device)\n",
        "        yb = yb.to(device)\n",
        "        logits = model_v2(xb)\n",
        "        preds = logits.argmax(dim=1)\n",
        "        ys.extend(yb.cpu().numpy().tolist())\n",
        "        ys_pred.extend(preds.cpu().numpy().tolist())\n",
        "\n",
        "test_acc_v2 = accuracy_score(ys, ys_pred)\n",
        "test_f1_v2 = f1_score(ys, ys_pred, average='macro')\n",
        "print(f\"[v2] Test accuracy: {test_acc_v2:.4f}, macro F1: {test_f1_v2:.4f}\")\n",
        "\n",
        "report_v2 = classification_report(ys, ys_pred, target_names=[idx_to_label[i] for i in range(len(labels))])\n",
        "print(report_v2)\n",
        "\n",
        "cm_v2 = confusion_matrix(ys, ys_pred)\n",
        "print('Confusion matrix shape:', cm_v2.shape)\n",
        "\n",
        "pd.DataFrame({\n",
        "    'path': list(X_test),\n",
        "    'true_label': [idx_to_label[int(i)] for i in ys],\n",
        "    'pred_label': [idx_to_label[int(i)] for i in ys_pred],\n",
        "}).to_csv(save_dir_v2 / 'test_predictions_v2.csv', index=False)\n",
        "\n",
        "print('Saved v2 predictions to', save_dir_v2 / 'test_predictions_v2.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fast(er) loaders for CPU, and lighter transforms if no CUDA\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "pin_memory_v2 = (device.type == 'cuda')\n",
        "num_workers_v2 = 0 if os.name == 'nt' else max(1, min(4, (os.cpu_count() or 2) - 1))\n",
        "\n",
        "if transforms is None:\n",
        "    raise RuntimeError('torchvision is required for v2 fast pipeline')\n",
        "\n",
        "if device.type == 'cuda':\n",
        "    train_tfms_v2_fast = train_tfms_v2\n",
        "    val_tfms_v2_fast = val_tfms_v2\n",
        "else:\n",
        "    # Lighter CPU transforms\n",
        "    train_tfms_v2_fast = transforms.Compose([\n",
        "        transforms.Grayscale(num_output_channels=CHANNELS_V2),\n",
        "        transforms.Resize(int(IMG_SIZE_V2*1.05)),\n",
        "        transforms.RandomCrop(IMG_SIZE_V2),\n",
        "        transforms.RandomRotation(6, fill=0),\n",
        "        transforms.RandomAffine(degrees=0, shear=6, translate=(0.03,0.03)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean_v2, std_v2),\n",
        "    ])\n",
        "    val_tfms_v2_fast = transforms.Compose([\n",
        "        transforms.Grayscale(num_output_channels=CHANNELS_V2),\n",
        "        transforms.Resize((IMG_SIZE_V2, IMG_SIZE_V2)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean_v2, std_v2),\n",
        "    ])\n",
        "\n",
        "train_ds_v2_fast = HandwritingDatasetV2(X_train, y_train, transform=train_tfms_v2_fast)\n",
        "val_ds_v2_fast   = HandwritingDatasetV2(X_val, y_val, transform=val_tfms_v2_fast)\n",
        "test_ds_v2_fast  = HandwritingDatasetV2(X_test, y_test, transform=val_tfms_v2_fast)\n",
        "\n",
        "# Rebuild sampler to match dataset length\n",
        "class_counts = Counter(y_train.tolist())\n",
        "num_classes = len(labels)\n",
        "class_freq = np.array([class_counts.get(i, 0) for i in range(num_classes)], dtype=np.float64)\n",
        "class_weights = 1.0 / np.maximum(class_freq, 1.0)\n",
        "class_weights = class_weights / class_weights.sum() * num_classes\n",
        "sample_weights = np.array([class_weights[y] for y in y_train], dtype=np.float64)\n",
        "sampler_fast = WeightedRandomSampler(weights=torch.DoubleTensor(sample_weights), num_samples=len(sample_weights), replacement=True)\n",
        "\n",
        "train_loader_v2_fast = DataLoader(train_ds_v2_fast, batch_size=BATCH_SIZE_V2, sampler=sampler_fast, num_workers=num_workers_v2, pin_memory=pin_memory_v2)\n",
        "val_loader_v2_fast   = DataLoader(val_ds_v2_fast, batch_size=BATCH_SIZE_V2, shuffle=False, num_workers=num_workers_v2, pin_memory=pin_memory_v2)\n",
        "test_loader_v2_fast  = DataLoader(test_ds_v2_fast, batch_size=BATCH_SIZE_V2, shuffle=False, num_workers=num_workers_v2, pin_memory=pin_memory_v2)\n",
        "\n",
        "len(train_ds_v2_fast), len(val_ds_v2_fast), len(test_ds_v2_fast), num_workers_v2, pin_memory_v2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# v2 training (fast) with head-only warmup then partial unfreeze, tqdm, new torch.amp API\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "# Update AMP API per warning\n",
        "use_amp = (device.type == 'cuda')\n",
        "scaler = torch.amp.GradScaler(device.type) if use_amp else None\n",
        "\n",
        "# Rebuild optimizer & scheduler\n",
        "for p in model_v2.parameters():\n",
        "    p.requires_grad = True\n",
        "\n",
        "# Head-only warmup: train final layer(s) first\n",
        "head_params = list(model_v2.fc.parameters()) if hasattr(model_v2, 'fc') else list(model_v2.classifier.parameters())\n",
        "backbone_params = [p for p in model_v2.parameters() if p not in head_params]\n",
        "for p in backbone_params:\n",
        "    p.requires_grad = False\n",
        "\n",
        "optimizer_v2 = torch.optim.AdamW(filter(lambda p: p.requires_grad, model_v2.parameters()), lr=3e-3, weight_decay=WEIGHT_DECAY_V2)\n",
        "steps_per_epoch = max(1, math.ceil(len(train_ds_v2_fast) / BATCH_SIZE_V2))\n",
        "scheduler_v2 = torch.optim.lr_scheduler.OneCycleLR(optimizer_v2, max_lr=3e-3, steps_per_epoch=steps_per_epoch,\n",
        "                                                   epochs=EPOCHS_V2, pct_start=0.2, div_factor=10.0, final_div_factor=10.0)\n",
        "\n",
        "criterion_v2 = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32, device=device),\n",
        "                                   label_smoothing=LABEL_SMOOTHING_V2)\n",
        "\n",
        "best_val_f1 = 0.0\n",
        "patience = 5\n",
        "pat = 0\n",
        "\n",
        "unfrozen = False\n",
        "\n",
        "for epoch in range(1, EPOCHS_V2+1):\n",
        "    model_v2.train()\n",
        "    running_loss = 0.0\n",
        "    pbar = tqdm(train_loader_v2_fast, desc=f\"[v2-fast] Epoch {epoch}/{EPOCHS_V2}\")\n",
        "    for xb, yb in pbar:\n",
        "        xb = xb.to(device)\n",
        "        yb = yb.to(device)\n",
        "        optimizer_v2.zero_grad(set_to_none=True)\n",
        "        if use_amp:\n",
        "            with torch.amp.autocast(device_type=device.type, enabled=True):\n",
        "                logits = model_v2(xb)\n",
        "                loss = criterion_v2(logits, yb)\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer_v2)\n",
        "            scaler.update()\n",
        "        else:\n",
        "            logits = model_v2(xb)\n",
        "            loss = criterion_v2(logits, yb)\n",
        "            loss.backward()\n",
        "            optimizer_v2.step()\n",
        "        scheduler_v2.step()\n",
        "        running_loss += loss.item()\n",
        "        pbar.set_postfix({'loss': f\"{running_loss / (pbar.n or 1):.4f}\"})\n",
        "    train_loss = running_loss / max(1, len(train_loader_v2_fast))\n",
        "\n",
        "    # Validate\n",
        "    model_v2.eval()\n",
        "    ys, ys_pred = [], []\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in val_loader_v2_fast:\n",
        "            xb = xb.to(device)\n",
        "            yb = yb.to(device)\n",
        "            if use_amp:\n",
        "                with torch.amp.autocast(device_type=device.type, enabled=True):\n",
        "                    logits = model_v2(xb)\n",
        "            else:\n",
        "                logits = model_v2(xb)\n",
        "            preds = logits.argmax(dim=1)\n",
        "            ys.extend(yb.cpu().numpy().tolist())\n",
        "            ys_pred.extend(preds.cpu().numpy().tolist())\n",
        "    val_acc = accuracy_score(ys, ys_pred)\n",
        "    val_f1 = f1_score(ys, ys_pred, average='macro')\n",
        "    print(f\"[v2-fast] Epoch {epoch:02d}/{EPOCHS_V2} - loss: {train_loss:.4f} - val_acc: {val_acc:.4f} - val_macro_f1: {val_f1:.4f}\")\n",
        "\n",
        "    # Unfreeze backbone after 3 epochs\n",
        "    if (not unfrozen) and epoch >= 3:\n",
        "        for p in backbone_params:\n",
        "            p.requires_grad = True\n",
        "        optimizer_v2 = torch.optim.AdamW(model_v2.parameters(), lr=1e-3, weight_decay=WEIGHT_DECAY_V2)\n",
        "        scheduler_v2 = torch.optim.lr_scheduler.OneCycleLR(optimizer_v2, max_lr=1e-3, steps_per_epoch=steps_per_epoch,\n",
        "                                                           epochs=max(5, EPOCHS_V2-epoch), pct_start=0.2, div_factor=10.0, final_div_factor=10.0)\n",
        "        unfrozen = True\n",
        "        print('Backbone unfrozen and optimizer reset with lower LR')\n",
        "\n",
        "    if val_f1 > best_val_f1:\n",
        "        best_val_f1 = val_f1\n",
        "        torch.save({'model_state': model_v2.state_dict(),\n",
        "                    'config': {'img_size': IMG_SIZE_V2, 'num_classes': len(labels), 'channels': CHANNELS_V2,\n",
        "                               'mean': mean_v2, 'std': std_v2, 'backbone': model_v2.__class__.__name__}},\n",
        "                   model_path_v2)\n",
        "        print(f\"Saved best v2 model (val_macro_f1={best_val_f1:.4f}) -> {model_path_v2}\")\n",
        "        pat = 0\n",
        "    else:\n",
        "        pat += 1\n",
        "        if pat >= patience:\n",
        "            print('Early stopping triggered')\n",
        "            break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# v2-fast test evaluation\n",
        "if model_path_v2.exists():\n",
        "    ckpt = torch.load(model_path_v2, map_location=device)\n",
        "    model_v2.load_state_dict(ckpt['model_state'])\n",
        "\n",
        "model_v2.eval()\n",
        "ys, ys_pred = [], []\n",
        "with torch.no_grad():\n",
        "    for xb, yb in test_loader_v2_fast:\n",
        "        xb = xb.to(device)\n",
        "        yb = yb.to(device)\n",
        "        logits = model_v2(xb)\n",
        "        preds = logits.argmax(dim=1)\n",
        "        ys.extend(yb.cpu().numpy().tolist())\n",
        "        ys_pred.extend(preds.cpu().numpy().tolist())\n",
        "\n",
        "test_acc_v2 = accuracy_score(ys, ys_pred)\n",
        "test_f1_v2 = f1_score(ys, ys_pred, average='macro')\n",
        "print(f\"[v2-fast] Test accuracy: {test_acc_v2:.4f}, macro F1: {test_f1_v2:.4f}\")\n",
        "\n",
        "pd.DataFrame({\n",
        "    'path': list(X_test),\n",
        "    'true_label': [idx_to_label[int(i)] for i in ys],\n",
        "    'pred_label': [idx_to_label[int(i)] for i in ys_pred],\n",
        "}).to_csv(save_dir_v2 / 'test_predictions_v2_fast.csv', index=False)\n",
        "\n",
        "print('Saved v2-fast predictions to', save_dir_v2 / 'test_predictions_v2_fast.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inference helper v2\n",
        "class InferenceModelV2:\n",
        "    def __init__(self, model_path: Path, labels_path: Path):\n",
        "        with open(labels_path, 'r') as f:\n",
        "            self.idx_to_label = {int(k):v for k,v in json.load(f).items()}\n",
        "        ckpt = torch.load(model_path, map_location=device)\n",
        "        cfg = ckpt.get('config', {})\n",
        "        self.img_size = cfg.get('img_size', IMG_SIZE_V2)\n",
        "        self.channels = cfg.get('channels', CHANNELS_V2)\n",
        "        self.mean = cfg.get('mean', [0.5]*self.channels)\n",
        "        self.std = cfg.get('std', [0.5]*self.channels)\n",
        "        # Build model\n",
        "        try:\n",
        "            self.model = build_resnet18_head(len(self.idx_to_label), pretrained=False).to(device)\n",
        "        except Exception:\n",
        "            self.model = model_v2.__class__(len(self.idx_to_label)).to(device)\n",
        "        self.model.load_state_dict(ckpt['model_state'])\n",
        "        self.model.eval()\n",
        "        # Transforms\n",
        "        self.tfms = transforms.Compose([\n",
        "            transforms.Grayscale(num_output_channels=self.channels),\n",
        "            transforms.Resize((self.img_size, self.img_size)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(self.mean, self.std),\n",
        "        ])\n",
        "    @torch.no_grad()\n",
        "    def predict(self, image_path: Path):\n",
        "        with Image.open(image_path) as img:\n",
        "            img = img.convert('L')\n",
        "            x = self.tfms(img).unsqueeze(0).to(device)\n",
        "        logits = self.model(x)\n",
        "        pred = logits.argmax(dim=1).item()\n",
        "        return self.idx_to_label[pred]\n",
        "\n",
        "# Example:\n",
        "# infer_v2 = InferenceModelV2(model_path_v2, labels_path_v2)\n",
        "# infer_v2.predict(Path('Assets/...'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# (Removed) Baseline inference — use InferenceModelV2 with artifacts_v2\n",
        "'Use InferenceModelV2(model_path_v2, labels_path_v2)'\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
