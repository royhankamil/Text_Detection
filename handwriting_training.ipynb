{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Handwriting Image Classifier (PyTorch)\n",
        "\n",
        "This notebook trains a classifier for handwriting images using the datasets in `Assets/`:\n",
        "- `augmented_images/`: directory-of-directories (each subfolder is a class)\n",
        "- `handwritten-english-characters-and-digits/`: has `train/` and `test/` splits (each has class subfolders)\n",
        "- `image_labels.csv`: has `filename,label` mapping; we'll search for the files within `Assets/`\n",
        "\n",
        "Flow:\n",
        "- Discover and merge samples from all three sources\n",
        "- Build stratified train/val/test splits\n",
        "- Train a pretrained ResNet18-based model (with a CPU-friendly v2-fast option)\n",
        "- Evaluate and save the best model and label mapping\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in c:\\users\\royha\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.3.1)\n",
            "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\royha\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2.2.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\royha\\appdata\\roaming\\python\\python313\\site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\royha\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\royha\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\royha\\appdata\\roaming\\python\\python313\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.0.1 -> 25.2\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "pip install pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU threads set to: 4\n"
          ]
        }
      ],
      "source": [
        "# System/perf settings\n",
        "import multiprocessing as mp\n",
        "from PIL import ImageFile\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "# Environment & imports\n",
        "import os, sys, json, random, math, time\n",
        "from pathlib import Path\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "# Cap CPU threads to avoid oversubscription\n",
        "try:\n",
        "    torch.set_num_threads(max(1, min(os.cpu_count() or 4, 8)))\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "print('CPU threads set to:', torch.get_num_threads())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "try:\n",
        "    import torchvision\n",
        "    from torchvision import transforms\n",
        "except Exception as e:\n",
        "    print(\"torchvision not found; attempting to continue with PIL-only transforms\")\n",
        "    torchvision = None\n",
        "    transforms = None\n",
        "\n",
        "ASSETS = Path('Assets')\n",
        "assert ASSETS.exists(), f\"Assets folder not found at {ASSETS.resolve()}\"\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "augmented_images: 13640 samples\n",
            "HED train: 2728 samples, HED test: 682 samples\n",
            "CSV samples resolved from image_labels.csv: 0 (missing: 13640)\n",
            "Total unique samples: 17050\n",
            "Num classes: 62\n",
            "Class distribution (top 20): [('0', 275), ('1', 275), ('2', 275), ('3', 275), ('4', 275), ('5', 275), ('6', 275), ('7', 275), ('8', 275), ('9', 275), ('a', 275), ('A_caps', 275), ('b', 275), ('B_caps', 275), ('c', 275), ('C_caps', 275), ('d', 275), ('D_caps', 275), ('e', 275), ('E_caps', 275)]\n"
          ]
        }
      ],
      "source": [
        "# Discover datasets\n",
        "from typing import List, Tuple\n",
        "\n",
        "IMG_EXTS = {'.png', '.jpg', '.jpeg', '.bmp', '.gif'}\n",
        "\n",
        "def list_images_in_dir_of_dirs(root: Path) -> List[Tuple[Path, str]]:\n",
        "    samples = []\n",
        "    if not root.exists():\n",
        "        return samples\n",
        "    for class_dir in sorted([p for p in root.iterdir() if p.is_dir()]):\n",
        "        label = class_dir.name\n",
        "        for p in class_dir.rglob('*'):\n",
        "            if p.suffix.lower() in IMG_EXTS and p.is_file():\n",
        "                samples.append((p, label))\n",
        "    return samples\n",
        "\n",
        "# 1) augmented_images\n",
        "aug_dir = ASSETS / 'augmented_images'\n",
        "aug_samples = list_images_in_dir_of_dirs(aug_dir)\n",
        "print(f\"augmented_images: {len(aug_samples)} samples\")\n",
        "\n",
        "# 2) handwritten-english-characters-and-digits/train and test\n",
        "hed_root = ASSETS / 'handwritten-english-characters-and-digits'\n",
        "hed_train = list_images_in_dir_of_dirs(hed_root / 'train')\n",
        "hed_test = list_images_in_dir_of_dirs(hed_root / 'test')\n",
        "print(f\"HED train: {len(hed_train)} samples, HED test: {len(hed_test)} samples\")\n",
        "\n",
        "# 3) image_labels.csv: filename,label\n",
        "csv_path = ASSETS / 'image_labels.csv'\n",
        "if not csv_path.exists():\n",
        "    alt_csv_path = ASSETS / 'image_label.csv'  # support alternate name\n",
        "    if alt_csv_path.exists():\n",
        "        csv_path = alt_csv_path\n",
        "\n",
        "csv_samples = []\n",
        "if csv_path.exists():\n",
        "    df = pd.read_csv(csv_path)\n",
        "    assert {'filename','label'}.issubset(df.columns)\n",
        "    # Index all images under Assets for filename lookup\n",
        "    all_imgs = {p.name: p for p in ASSETS.rglob('*') if p.suffix.lower() in IMG_EXTS}\n",
        "    missing = 0\n",
        "    for _, row in df.iterrows():\n",
        "        fname = str(row['filename'])\n",
        "        label = str(row['label'])\n",
        "        if fname in all_imgs:\n",
        "            csv_samples.append((all_imgs[fname], label))\n",
        "        else:\n",
        "            # Try to search by suffix match if duplicates are unlikely\n",
        "            matches = [p for n,p in all_imgs.items() if n.endswith(fname)]\n",
        "            if len(matches) == 1:\n",
        "                csv_samples.append((matches[0], label))\n",
        "            else:\n",
        "                missing += 1\n",
        "    print(f\"CSV samples resolved from {csv_path.name}: {len(csv_samples)} (missing: {missing})\")\n",
        "else:\n",
        "    print(\"image_labels.csv not found; skipping CSV source\")\n",
        "\n",
        "# Merge all; if duplicates appear, prefer explicit CSV labels > hed > aug\n",
        "# Use image absolute path as key\n",
        "merged = {}\n",
        "for p,l in aug_samples:\n",
        "    merged[str(p.resolve())] = (p, l)\n",
        "for p,l in hed_train + hed_test:\n",
        "    merged[str(p.resolve())] = (p, l)\n",
        "for p,l in csv_samples:\n",
        "    merged[str(p.resolve())] = (p, l)\n",
        "\n",
        "all_samples = list(merged.values())\n",
        "print(f\"Total unique samples: {len(all_samples)}\")\n",
        "\n",
        "# Map labels to indices\n",
        "labels = sorted(sorted({l for _, l in all_samples}))\n",
        "label_to_idx = {l:i for i,l in enumerate(labels)}\n",
        "idx_to_label = {i:l for l,i in label_to_idx.items()}\n",
        "print(f\"Num classes: {len(labels)}\")\n",
        "\n",
        "# Class distribution\n",
        "counts = Counter([l for _,l in all_samples])\n",
        "print(\"Class distribution (top 20):\", counts.most_common(20))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "13640 1705 1705\n"
          ]
        }
      ],
      "source": [
        "# Train/Val/Test split (stratified)\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "random_seed = 42\n",
        "rng = np.random.RandomState(random_seed)\n",
        "\n",
        "paths = np.array([str(p) for p,_ in all_samples])\n",
        "labels_arr = np.array([label_to_idx[l] for _,l in all_samples])\n",
        "\n",
        "# If HED has an explicit test set, we already included it. We'll still split overall\n",
        "# into train/val/test=0.8/0.1/0.1 stratified.\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(\n",
        "    paths, labels_arr, test_size=0.1, random_state=random_seed, stratify=labels_arr\n",
        ")\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_temp, y_temp, test_size=0.1111, random_state=random_seed, stratify=y_temp\n",
        ")  # 0.8 * 0.1111 ≈ 0.0889 so final ≈ 80/10/10\n",
        "\n",
        "print(len(X_train), len(X_val), len(X_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "# (Removed) Baseline transforms and dataset — superseded by v2. Keeping small helper for compatibility.\n",
        "IMG_SIZE = 64\n",
        "basic_train_tfms = None\n",
        "basic_val_tfms = None\n",
        "\n",
        "class HandwritingDataset(Dataset):\n",
        "    def __init__(self, paths: np.ndarray, labels: np.ndarray, transform=None):\n",
        "        self.paths = paths\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "    def __len__(self):\n",
        "        return len(self.paths)\n",
        "    def __getitem__(self, idx):\n",
        "        p = Path(self.paths[idx])\n",
        "        y = int(self.labels[idx])\n",
        "        with Image.open(p) as img:\n",
        "            img = img.convert('L')\n",
        "            if self.transform is not None:\n",
        "                x = self.transform(img)\n",
        "            else:\n",
        "                # minimal fallback\n",
        "                img = img.resize((IMG_SIZE, IMG_SIZE))\n",
        "                arr = np.array(img, dtype=np.float32)/255.0\n",
        "                arr = arr[None, ...]\n",
        "                x = torch.from_numpy(arr)\n",
        "        return x, y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Use cells 17–19 for training/eval loaders.'"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# (Removed) Baseline dataloaders — use v2 or v2-fast loaders below\n",
        "'Use cells 17–19 for training/eval loaders.'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Use cells 11–19 for model, training and evaluation.'"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# (Removed) SmallCNN baseline — using pretrained ResNet18 (v2)\n",
        "'Use cells 11–19 for model, training and evaluation.'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Use cells 18–19 (or 15–16) for training and evaluation.'"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# (Removed) Baseline training utilities — replaced by v2 training below\n",
        "'Use cells 18–19 (or 15–16) for training and evaluation.'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Use the v2 model: artifacts_v2/handwriting_resnet18_best.pt'"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# (Removed) Baseline evaluation — use v2/v2-fast evaluation cells (16 or 19)\n",
        "'Use the v2 model: artifacts_v2/handwriting_resnet18_best.pt'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Improved configuration (v2)\n",
        "IMG_SIZE_V2 = 128\n",
        "CHANNELS_V2 = 3  # use 3-channel to leverage pretrained backbones\n",
        "EPOCHS_V2 = 30\n",
        "BATCH_SIZE_V2 = 128\n",
        "WEIGHT_DECAY_V2 = 1e-4\n",
        "LABEL_SMOOTHING_V2 = 0.1\n",
        "RANDOM_SEED_V2 = 42\n",
        "USE_PRETRAINED_V2 = True  # will fallback automatically if not available\n",
        "\n",
        "# Quick mode for CPU/fast runs\n",
        "QUICK_MODE = True if device.type == 'cpu' else False\n",
        "# Override defaults when quick mode is on\n",
        "if QUICK_MODE:\n",
        "    IMG_SIZE_V2 = 96\n",
        "    EPOCHS_V2 = 5\n",
        "    BATCH_SIZE_V2 = 32\n",
        "USE_WEIGHTED_SAMPLER = not QUICK_MODE\n",
        "FREEZE_BACKBONE_ONLY = QUICK_MODE  # keep backbone frozen for speed in quick mode\n",
        "\n",
        "rng_v2 = np.random.RandomState(RANDOM_SEED_V2)\n",
        "\n",
        "# Create artifacts dir for v2\n",
        "save_dir_v2 = Path('artifacts_v2')\n",
        "save_dir_v2.mkdir(exist_ok=True)\n",
        "model_path_v2 = save_dir_v2 / 'handwriting_resnet18_best.pt'\n",
        "labels_path_v2 = save_dir_v2 / 'labels.json'\n",
        "with open(labels_path_v2, 'w') as f:\n",
        "    json.dump(idx_to_label, f, indent=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using ResNet18 backbone for v2. Imagenet norm: True\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(11208318, 'ResNet')"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Model v2: ResNet18 backbone with pretrained fallback\n",
        "from typing import Optional\n",
        "\n",
        "USE_IMAGENET_NORM_V2 = False\n",
        "\n",
        "def build_resnet18_head(num_classes: int, pretrained: bool = True) -> nn.Module:\n",
        "    global USE_IMAGENET_NORM_V2\n",
        "    if torchvision is None:\n",
        "        raise RuntimeError('torchvision not available')\n",
        "    try:\n",
        "        try:\n",
        "            # torchvision >=0.13\n",
        "            from torchvision.models import resnet18, ResNet18_Weights\n",
        "            weights = ResNet18_Weights.IMAGENET1K_V1 if pretrained else None\n",
        "            model = resnet18(weights=weights)\n",
        "            USE_IMAGENET_NORM_V2 = pretrained\n",
        "        except Exception:\n",
        "            # older API\n",
        "            from torchvision.models import resnet18\n",
        "            model = resnet18(pretrained=pretrained)\n",
        "            USE_IMAGENET_NORM_V2 = pretrained\n",
        "    except Exception as e:\n",
        "        print('Pretrained resnet18 unavailable, falling back to randomly initialized.')\n",
        "        from torchvision.models import resnet18\n",
        "        model = resnet18(pretrained=False)\n",
        "        USE_IMAGENET_NORM_V2 = False\n",
        "    # Adjust first layer to accept 3-channel grayscale (we feed 3-channel grayscale, so no change needed)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, num_classes)\n",
        "    return model\n",
        "\n",
        "try:\n",
        "    model_v2 = build_resnet18_head(len(labels), pretrained=USE_PRETRAINED_V2).to(device)\n",
        "    print('Using ResNet18 backbone for v2. Imagenet norm:', USE_IMAGENET_NORM_V2)\n",
        "except Exception as e:\n",
        "    print('Falling back to SmallCNN due to error:', e)\n",
        "    CHANNELS_V2 = 1\n",
        "    class DeeperCNN(nn.Module):\n",
        "        def __init__(self, num_classes: int):\n",
        "            super().__init__()\n",
        "            self.features = nn.Sequential(\n",
        "                nn.Conv2d(1, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(),\n",
        "                nn.Conv2d(64, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(),\n",
        "                nn.MaxPool2d(2),\n",
        "                nn.Conv2d(64, 128, 3, padding=1), nn.BatchNorm2d(128), nn.ReLU(),\n",
        "                nn.Conv2d(128, 128, 3, padding=1), nn.BatchNorm2d(128), nn.ReLU(),\n",
        "                nn.MaxPool2d(2),\n",
        "                nn.Conv2d(128, 256, 3, padding=1), nn.BatchNorm2d(256), nn.ReLU(),\n",
        "                nn.MaxPool2d(2),\n",
        "            )\n",
        "            fsize = IMG_SIZE_V2 // 8\n",
        "            self.classifier = nn.Sequential(\n",
        "                nn.Dropout(0.4), nn.Linear(256*fsize*fsize, 512), nn.ReLU(),\n",
        "                nn.Dropout(0.4), nn.Linear(512, num_classes)\n",
        "            )\n",
        "        def forward(self, x):\n",
        "            x = self.features(x)\n",
        "            x = torch.flatten(x, 1)\n",
        "            return self.classifier(x)\n",
        "    model_v2 = DeeperCNN(len(labels)).to(device)\n",
        "    USE_IMAGENET_NORM_V2 = False\n",
        "\n",
        "sum(p.numel() for p in model_v2.parameters() if p.requires_grad), model_v2.__class__.__name__\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Transforms v2 (with optional dataset normalization)\n",
        "from typing import Tuple\n",
        "\n",
        "def compute_mean_std(paths_subset: np.ndarray, channels: int, sample_size: int = 1024) -> Tuple[list, list]:\n",
        "    sample_paths = paths_subset.copy()\n",
        "    if len(sample_paths) > sample_size:\n",
        "        sample_paths = rng_v2.choice(sample_paths, size=sample_size, replace=False)\n",
        "    means = np.zeros(channels, dtype=np.float64)\n",
        "    sq_means = np.zeros(channels, dtype=np.float64)\n",
        "    n_pix_total = 0\n",
        "    for p in sample_paths:\n",
        "        with Image.open(p) as img:\n",
        "            img = img.convert('L')  # grayscale base\n",
        "            img = img.resize((IMG_SIZE_V2, IMG_SIZE_V2))\n",
        "            arr = np.array(img, dtype=np.float32) / 255.0\n",
        "            if channels == 1:\n",
        "                arr_c = arr[None, ...]\n",
        "            else:\n",
        "                arr_c = np.stack([arr, arr, arr], axis=0)\n",
        "            n_pix = arr_c.shape[1] * arr_c.shape[2]\n",
        "            means += arr_c.reshape(channels, -1).sum(axis=1)\n",
        "            sq_means += (arr_c.reshape(channels, -1) ** 2).sum(axis=1)\n",
        "            n_pix_total += n_pix\n",
        "    means /= n_pix_total\n",
        "    stds = np.sqrt(sq_means / n_pix_total - means**2)\n",
        "    return means.tolist(), stds.tolist()\n",
        "\n",
        "if transforms is None:\n",
        "    raise RuntimeError('torchvision is required for v2 pipeline transforms')\n",
        "\n",
        "if USE_IMAGENET_NORM_V2:\n",
        "    # ImageNet mean/std\n",
        "    mean_v2 = [0.485, 0.456, 0.406]\n",
        "    std_v2 = [0.229, 0.224, 0.225]\n",
        "else:\n",
        "    mean_v2, std_v2 = compute_mean_std(X_train, CHANNELS_V2, sample_size=1024)\n",
        "\n",
        "train_tfms_v2 = transforms.Compose([\n",
        "    transforms.Grayscale(num_output_channels=CHANNELS_V2),\n",
        "    transforms.Resize(int(IMG_SIZE_V2*1.1)),\n",
        "    transforms.RandomCrop(IMG_SIZE_V2),\n",
        "    transforms.RandomRotation(8, fill=0),\n",
        "    transforms.RandomAffine(degrees=0, shear=8, translate=(0.05,0.05)),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
        "    transforms.RandomPerspective(distortion_scale=0.2, p=0.3),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean_v2, std_v2),\n",
        "])\n",
        "\n",
        "val_tfms_v2 = transforms.Compose([\n",
        "    transforms.Grayscale(num_output_channels=CHANNELS_V2),\n",
        "    transforms.Resize((IMG_SIZE_V2, IMG_SIZE_V2)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean_v2, std_v2),\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(13640,\n",
              " 1705,\n",
              " 1705,\n",
              " [(34, 220),\n",
              "  (42, 220),\n",
              "  (52, 220),\n",
              "  (53, 220),\n",
              "  (37, 220),\n",
              "  (47, 220),\n",
              "  (50, 220),\n",
              "  (17, 220),\n",
              "  (55, 220),\n",
              "  (51, 220)])"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "# Dataset & DataLoaders v2 with optional class balancing\n",
        "from torch.utils.data import WeightedRandomSampler\n",
        "\n",
        "class HandwritingDatasetV2(Dataset):\n",
        "    def __init__(self, paths: np.ndarray, labels: np.ndarray, transform=None):\n",
        "        self.paths = paths\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "    def __len__(self):\n",
        "        return len(self.paths)\n",
        "    def __getitem__(self, idx):\n",
        "        p = Path(self.paths[idx])\n",
        "        y = int(self.labels[idx])\n",
        "        with Image.open(p) as img:\n",
        "            img = img.convert('L')\n",
        "            x = (self.transform or val_tfms_v2)(img)\n",
        "        return x, y\n",
        "\n",
        "train_ds_v2 = HandwritingDatasetV2(X_train, y_train, transform=train_tfms_v2)\n",
        "val_ds_v2   = HandwritingDatasetV2(X_val, y_val, transform=val_tfms_v2)\n",
        "test_ds_v2  = HandwritingDatasetV2(X_test, y_test, transform=val_tfms_v2)\n",
        "\n",
        "# Class weights for sampler and loss\n",
        "class_counts = Counter(y_train.tolist())\n",
        "num_classes = len(labels)\n",
        "class_freq = np.array([class_counts.get(i, 0) for i in range(num_classes)], dtype=np.float64)\n",
        "class_weights = 1.0 / np.maximum(class_freq, 1.0)\n",
        "class_weights = class_weights / class_weights.sum() * num_classes\n",
        "\n",
        "# DataLoaders: optionally use weighted sampler\n",
        "num_workers_v2 = 0 if os.name == 'nt' else 2\n",
        "if USE_WEIGHTED_SAMPLER:\n",
        "    sample_weights = np.array([class_weights[y] for y in y_train], dtype=np.float64)\n",
        "    sampler = WeightedRandomSampler(weights=torch.DoubleTensor(sample_weights), num_samples=len(sample_weights), replacement=True)\n",
        "    train_loader_v2 = DataLoader(train_ds_v2, batch_size=BATCH_SIZE_V2, sampler=sampler, num_workers=num_workers_v2, pin_memory=(device.type=='cuda'))\n",
        "else:\n",
        "    train_loader_v2 = DataLoader(train_ds_v2, batch_size=BATCH_SIZE_V2, shuffle=True, num_workers=num_workers_v2, pin_memory=(device.type=='cuda'))\n",
        "val_loader_v2   = DataLoader(val_ds_v2, batch_size=BATCH_SIZE_V2, shuffle=False, num_workers=num_workers_v2)\n",
        "test_loader_v2  = DataLoader(test_ds_v2, batch_size=BATCH_SIZE_V2, shuffle=False, num_workers=num_workers_v2)\n",
        "\n",
        "len(train_ds_v2), len(val_ds_v2), len(test_ds_v2), class_counts.most_common(10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\royha\\AppData\\Local\\Temp\\ipykernel_18364\\2646473394.py:12: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler(enabled=(device.type=='cuda'))\n",
            "c:\\Users\\royha\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "C:\\Users\\royha\\AppData\\Local\\Temp\\ipykernel_18364\\2646473394.py:28: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=(device.type=='cuda')):\n",
            "C:\\Users\\royha\\AppData\\Local\\Temp\\ipykernel_18364\\2646473394.py:46: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[v2] Epoch 01/30 - loss: 1.7866 - val_acc: 0.8182 - val_macro_f1: 0.8120\n",
            "Saved best v2 model (val_macro_f1=0.8120) -> artifacts_v2\\handwriting_resnet18_best.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\royha\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "C:\\Users\\royha\\AppData\\Local\\Temp\\ipykernel_18364\\2646473394.py:28: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=(device.type=='cuda')):\n",
            "C:\\Users\\royha\\AppData\\Local\\Temp\\ipykernel_18364\\2646473394.py:46: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[v2] Epoch 02/30 - loss: 1.1772 - val_acc: 0.7015 - val_macro_f1: 0.7107\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\royha\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "C:\\Users\\royha\\AppData\\Local\\Temp\\ipykernel_18364\\2646473394.py:28: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=(device.type=='cuda')):\n",
            "C:\\Users\\royha\\AppData\\Local\\Temp\\ipykernel_18364\\2646473394.py:46: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[v2] Epoch 03/30 - loss: 1.1891 - val_acc: 0.6933 - val_macro_f1: 0.6866\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\royha\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "C:\\Users\\royha\\AppData\\Local\\Temp\\ipykernel_18364\\2646473394.py:28: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=(device.type=='cuda')):\n",
            "C:\\Users\\royha\\AppData\\Local\\Temp\\ipykernel_18364\\2646473394.py:46: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[v2] Epoch 04/30 - loss: 1.2044 - val_acc: 0.7730 - val_macro_f1: 0.7638\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\royha\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "C:\\Users\\royha\\AppData\\Local\\Temp\\ipykernel_18364\\2646473394.py:28: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=(device.type=='cuda')):\n"
          ]
        }
      ],
      "source": [
        "# Training loop v2 with AMP, label smoothing, AdamW, OneCycleLR, early stopping on macro F1\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "criterion_v2 = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32, device=device),\n",
        "                                   label_smoothing=LABEL_SMOOTHING_V2)\n",
        "optimizer_v2 = torch.optim.AdamW(model_v2.parameters(), lr=3e-3, weight_decay=WEIGHT_DECAY_V2)\n",
        "steps_per_epoch = max(1, math.ceil(len(train_ds_v2) / BATCH_SIZE_V2))\n",
        "scheduler_v2 = torch.optim.lr_scheduler.OneCycleLR(optimizer_v2, max_lr=3e-3, steps_per_epoch=steps_per_epoch,\n",
        "                                                   epochs=EPOCHS_V2, pct_start=0.2, div_factor=10.0, final_div_factor=10.0)\n",
        "\n",
        "scaler = GradScaler(enabled=(device.type=='cuda'))\n",
        "\n",
        "best_val_f1 = 0.0\n",
        "patience = 6\n",
        "pat = 0\n",
        "\n",
        "history = {'epoch': [], 'train_loss': [], 'val_acc': [], 'val_f1': []}\n",
        "\n",
        "for epoch in range(1, EPOCHS_V2+1):\n",
        "    model_v2.train()\n",
        "    running_loss = 0.0\n",
        "    b = 0\n",
        "    for xb, yb in train_loader_v2:\n",
        "        xb = xb.to(device, non_blocking=True)\n",
        "        yb = yb.to(device, non_blocking=True)\n",
        "        optimizer_v2.zero_grad(set_to_none=True)\n",
        "        with autocast(enabled=(device.type=='cuda')):\n",
        "            logits = model_v2(xb)\n",
        "            loss = criterion_v2(logits, yb)\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer_v2)\n",
        "        scaler.update()\n",
        "        scheduler_v2.step()\n",
        "        running_loss += loss.item()\n",
        "        b += 1\n",
        "    train_loss = running_loss / max(1, b)\n",
        "\n",
        "    # Validate\n",
        "    model_v2.eval()\n",
        "    ys, ys_pred = [], []\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in val_loader_v2:\n",
        "            xb = xb.to(device)\n",
        "            yb = yb.to(device)\n",
        "            with autocast(enabled=(device.type=='cuda')):\n",
        "                logits = model_v2(xb)\n",
        "            preds = logits.argmax(dim=1)\n",
        "            ys.extend(yb.cpu().numpy().tolist())\n",
        "            ys_pred.extend(preds.cpu().numpy().tolist())\n",
        "    val_acc = accuracy_score(ys, ys_pred)\n",
        "    val_f1 = f1_score(ys, ys_pred, average='macro')\n",
        "    history['epoch'].append(epoch)\n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['val_acc'].append(val_acc)\n",
        "    history['val_f1'].append(val_f1)\n",
        "\n",
        "    print(f\"[v2] Epoch {epoch:02d}/{EPOCHS_V2} - loss: {train_loss:.4f} - val_acc: {val_acc:.4f} - val_macro_f1: {val_f1:.4f}\")\n",
        "\n",
        "    if val_f1 > best_val_f1:\n",
        "        best_val_f1 = val_f1\n",
        "        torch.save({'model_state': model_v2.state_dict(),\n",
        "                    'config': {'img_size': IMG_SIZE_V2, 'num_classes': len(labels), 'channels': CHANNELS_V2,\n",
        "                               'mean': mean_v2, 'std': std_v2, 'backbone': model_v2.__class__.__name__}},\n",
        "                   model_path_v2)\n",
        "        print(f\"Saved best v2 model (val_macro_f1={best_val_f1:.4f}) -> {model_path_v2}\")\n",
        "        pat = 0\n",
        "    else:\n",
        "        pat += 1\n",
        "        if pat >= patience:\n",
        "            print('Early stopping triggered')\n",
        "            break\n",
        "\n",
        "json.dump(history, open(save_dir_v2 / 'train_history.json', 'w'), indent=2)\n",
        "print('Best v2 val macro F1:', best_val_f1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test evaluation v2\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "if model_path_v2.exists():\n",
        "    ckpt = torch.load(model_path_v2, map_location=device)\n",
        "    model_v2.load_state_dict(ckpt['model_state'])\n",
        "\n",
        "model_v2.eval()\n",
        "ys, ys_pred = [], []\n",
        "with torch.no_grad():\n",
        "    for xb, yb in test_loader_v2:\n",
        "        xb = xb.to(device)\n",
        "        yb = yb.to(device)\n",
        "        logits = model_v2(xb)\n",
        "        preds = logits.argmax(dim=1)\n",
        "        ys.extend(yb.cpu().numpy().tolist())\n",
        "        ys_pred.extend(preds.cpu().numpy().tolist())\n",
        "\n",
        "test_acc_v2 = accuracy_score(ys, ys_pred)\n",
        "test_f1_v2 = f1_score(ys, ys_pred, average='macro')\n",
        "print(f\"[v2] Test accuracy: {test_acc_v2:.4f}, macro F1: {test_f1_v2:.4f}\")\n",
        "\n",
        "report_v2 = classification_report(ys, ys_pred, target_names=[idx_to_label[i] for i in range(len(labels))])\n",
        "print(report_v2)\n",
        "\n",
        "cm_v2 = confusion_matrix(ys, ys_pred)\n",
        "print('Confusion matrix shape:', cm_v2.shape)\n",
        "\n",
        "pd.DataFrame({\n",
        "    'path': list(X_test),\n",
        "    'true_label': [idx_to_label[int(i)] for i in ys],\n",
        "    'pred_label': [idx_to_label[int(i)] for i in ys_pred],\n",
        "}).to_csv(save_dir_v2 / 'test_predictions_v2.csv', index=False)\n",
        "\n",
        "print('Saved v2 predictions to', save_dir_v2 / 'test_predictions_v2.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test evaluation v2\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "if model_path_v2.exists():\n",
        "    ckpt = torch.load(model_path_v2, map_location=device)\n",
        "    model_v2.load_state_dict(ckpt['model_state'])\n",
        "\n",
        "model_v2.eval()\n",
        "ys, ys_pred = [], []\n",
        "with torch.no_grad():\n",
        "    for xb, yb in test_loader_v2:\n",
        "        xb = xb.to(device)\n",
        "        yb = yb.to(device)\n",
        "        logits = model_v2(xb)\n",
        "        preds = logits.argmax(dim=1)\n",
        "        ys.extend(yb.cpu().numpy().tolist())\n",
        "        ys_pred.extend(preds.cpu().numpy().tolist())\n",
        "\n",
        "test_acc_v2 = accuracy_score(ys, ys_pred)\n",
        "test_f1_v2 = f1_score(ys, ys_pred, average='macro')\n",
        "print(f\"[v2] Test accuracy: {test_acc_v2:.4f}, macro F1: {test_f1_v2:.4f}\")\n",
        "\n",
        "report_v2 = classification_report(ys, ys_pred, target_names=[idx_to_label[i] for i in range(len(labels))])\n",
        "print(report_v2)\n",
        "\n",
        "cm_v2 = confusion_matrix(ys, ys_pred)\n",
        "print('Confusion matrix shape:', cm_v2.shape)\n",
        "\n",
        "pd.DataFrame({\n",
        "    'path': list(X_test),\n",
        "    'true_label': [idx_to_label[int(i)] for i in ys],\n",
        "    'pred_label': [idx_to_label[int(i)] for i in ys_pred],\n",
        "}).to_csv(save_dir_v2 / 'test_predictions_v2.csv', index=False)\n",
        "\n",
        "print('Saved v2 predictions to', save_dir_v2 / 'test_predictions_v2.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test evaluation v2\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "if model_path_v2.exists():\n",
        "    ckpt = torch.load(model_path_v2, map_location=device)\n",
        "    model_v2.load_state_dict(ckpt['model_state'])\n",
        "\n",
        "model_v2.eval()\n",
        "ys, ys_pred = [], []\n",
        "with torch.no_grad():\n",
        "    for xb, yb in test_loader_v2:\n",
        "        xb = xb.to(device)\n",
        "        yb = yb.to(device)\n",
        "        logits = model_v2(xb)\n",
        "        preds = logits.argmax(dim=1)\n",
        "        ys.extend(yb.cpu().numpy().tolist())\n",
        "        ys_pred.extend(preds.cpu().numpy().tolist())\n",
        "\n",
        "test_acc_v2 = accuracy_score(ys, ys_pred)\n",
        "test_f1_v2 = f1_score(ys, ys_pred, average='macro')\n",
        "print(f\"[v2] Test accuracy: {test_acc_v2:.4f}, macro F1: {test_f1_v2:.4f}\")\n",
        "\n",
        "report_v2 = classification_report(ys, ys_pred, target_names=[idx_to_label[i] for i in range(len(labels))])\n",
        "print(report_v2)\n",
        "\n",
        "cm_v2 = confusion_matrix(ys, ys_pred)\n",
        "print('Confusion matrix shape:', cm_v2.shape)\n",
        "\n",
        "pd.DataFrame({\n",
        "    'path': list(X_test),\n",
        "    'true_label': [idx_to_label[int(i)] for i in ys],\n",
        "    'pred_label': [idx_to_label[int(i)] for i in ys_pred],\n",
        "}).to_csv(save_dir_v2 / 'test_predictions_v2.csv', index=False)\n",
        "\n",
        "print('Saved v2 predictions to', save_dir_v2 / 'test_predictions_v2.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fast(er) loaders for CPU, and lighter transforms if no CUDA\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "pin_memory_v2 = (device.type == 'cuda')\n",
        "num_workers_v2 = 0 if os.name == 'nt' else max(1, min(4, (os.cpu_count() or 2) - 1))\n",
        "\n",
        "if transforms is None:\n",
        "    raise RuntimeError('torchvision is required for v2 fast pipeline')\n",
        "\n",
        "if device.type == 'cuda':\n",
        "    train_tfms_v2_fast = train_tfms_v2\n",
        "    val_tfms_v2_fast = val_tfms_v2\n",
        "else:\n",
        "    # Lighter CPU transforms\n",
        "    train_tfms_v2_fast = transforms.Compose([\n",
        "        transforms.Grayscale(num_output_channels=CHANNELS_V2),\n",
        "        transforms.Resize(int(IMG_SIZE_V2*1.05)),\n",
        "        transforms.RandomCrop(IMG_SIZE_V2),\n",
        "        transforms.RandomRotation(6, fill=0),\n",
        "        transforms.RandomAffine(degrees=0, shear=6, translate=(0.03,0.03)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean_v2, std_v2),\n",
        "    ])\n",
        "    val_tfms_v2_fast = transforms.Compose([\n",
        "        transforms.Grayscale(num_output_channels=CHANNELS_V2),\n",
        "        transforms.Resize((IMG_SIZE_V2, IMG_SIZE_V2)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean_v2, std_v2),\n",
        "    ])\n",
        "\n",
        "train_ds_v2_fast = HandwritingDatasetV2(X_train, y_train, transform=train_tfms_v2_fast)\n",
        "val_ds_v2_fast   = HandwritingDatasetV2(X_val, y_val, transform=val_tfms_v2_fast)\n",
        "test_ds_v2_fast  = HandwritingDatasetV2(X_test, y_test, transform=val_tfms_v2_fast)\n",
        "\n",
        "# Rebuild sampler to match dataset length\n",
        "class_counts = Counter(y_train.tolist())\n",
        "num_classes = len(labels)\n",
        "class_freq = np.array([class_counts.get(i, 0) for i in range(num_classes)], dtype=np.float64)\n",
        "class_weights = 1.0 / np.maximum(class_freq, 1.0)\n",
        "class_weights = class_weights / class_weights.sum() * num_classes\n",
        "\n",
        "# Build loaders (use weighted sampler only if enabled)\n",
        "if USE_WEIGHTED_SAMPLER:\n",
        "    sample_weights = np.array([class_weights[y] for y in y_train], dtype=np.float64)\n",
        "    sampler_fast = WeightedRandomSampler(weights=torch.DoubleTensor(sample_weights), num_samples=len(sample_weights), replacement=True)\n",
        "    train_loader_v2_fast = DataLoader(train_ds_v2_fast, batch_size=BATCH_SIZE_V2, sampler=sampler_fast, num_workers=num_workers_v2, pin_memory=pin_memory_v2)\n",
        "else:\n",
        "    train_loader_v2_fast = DataLoader(train_ds_v2_fast, batch_size=BATCH_SIZE_V2, shuffle=True, num_workers=num_workers_v2, pin_memory=pin_memory_v2)\n",
        "\n",
        "val_loader_v2_fast   = DataLoader(val_ds_v2_fast, batch_size=BATCH_SIZE_V2, shuffle=False, num_workers=num_workers_v2, pin_memory=pin_memory_v2)\n",
        "test_loader_v2_fast  = DataLoader(test_ds_v2_fast, batch_size=BATCH_SIZE_V2, shuffle=False, num_workers=num_workers_v2, pin_memory=pin_memory_v2)\n",
        "\n",
        "len(train_ds_v2_fast), len(val_ds_v2_fast), len(test_ds_v2_fast), num_workers_v2, pin_memory_v2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# v2 training (fast) with head-only warmup then optional unfreeze, tqdm, new torch.amp API\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "# Update AMP API per warning\n",
        "use_amp = (device.type == 'cuda')\n",
        "scaler = torch.amp.GradScaler(device.type) if use_amp else None\n",
        "\n",
        "# Rebuild optimizer & scheduler\n",
        "for p in model_v2.parameters():\n",
        "    p.requires_grad = True\n",
        "\n",
        "# Head-only warmup: train final layer(s) first\n",
        "head_params = list(model_v2.fc.parameters()) if hasattr(model_v2, 'fc') else list(model_v2.classifier.parameters())\n",
        "backbone_params = [p for p in model_v2.parameters() if p not in head_params]\n",
        "for p in backbone_params:\n",
        "    p.requires_grad = False if QUICK_MODE else True\n",
        "\n",
        "optimizer_v2 = torch.optim.AdamW(filter(lambda p: p.requires_grad, model_v2.parameters()), lr=3e-3, weight_decay=WEIGHT_DECAY_V2)\n",
        "steps_per_epoch = max(1, math.ceil(len(train_ds_v2_fast) / BATCH_SIZE_V2))\n",
        "scheduler_v2 = torch.optim.lr_scheduler.OneCycleLR(optimizer_v2, max_lr=3e-3, steps_per_epoch=steps_per_epoch,\n",
        "                                                   epochs=EPOCHS_V2, pct_start=0.2, div_factor=10.0, final_div_factor=10.0)\n",
        "\n",
        "criterion_v2 = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32, device=device),\n",
        "                                   label_smoothing=LABEL_SMOOTHING_V2)\n",
        "\n",
        "best_val_f1 = 0.0\n",
        "patience = 3 if QUICK_MODE else 5\n",
        "pat = 0\n",
        "\n",
        "unfrozen = False\n",
        "\n",
        "for epoch in range(1, EPOCHS_V2+1):\n",
        "    model_v2.train()\n",
        "    running_loss = 0.0\n",
        "    pbar = tqdm(train_loader_v2_fast, desc=f\"[v2-fast] Epoch {epoch}/{EPOCHS_V2}\")\n",
        "    for xb, yb in pbar:\n",
        "        xb = xb.to(device)\n",
        "        yb = yb.to(device)\n",
        "        optimizer_v2.zero_grad(set_to_none=True)\n",
        "        if use_amp:\n",
        "            with torch.amp.autocast(device_type=device.type, enabled=True):\n",
        "                logits = model_v2(xb)\n",
        "                loss = criterion_v2(logits, yb)\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer_v2)\n",
        "            scaler.update()\n",
        "        else:\n",
        "            logits = model_v2(xb)\n",
        "            loss = criterion_v2(logits, yb)\n",
        "            loss.backward()\n",
        "            optimizer_v2.step()\n",
        "        scheduler_v2.step()\n",
        "        running_loss += loss.item()\n",
        "        pbar.set_postfix({'loss': f\"{running_loss / (pbar.n or 1):.4f}\"})\n",
        "    train_loss = running_loss / max(1, len(train_loader_v2_fast))\n",
        "\n",
        "    # Validate\n",
        "    model_v2.eval()\n",
        "    ys, ys_pred = [], []\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in val_loader_v2_fast:\n",
        "            xb = xb.to(device)\n",
        "            yb = yb.to(device)\n",
        "            if use_amp:\n",
        "                with torch.amp.autocast(device_type=device.type, enabled=True):\n",
        "                    logits = model_v2(xb)\n",
        "            else:\n",
        "                logits = model_v2(xb)\n",
        "            preds = logits.argmax(dim=1)\n",
        "            ys.extend(yb.cpu().numpy().tolist())\n",
        "            ys_pred.extend(preds.cpu().numpy().tolist())\n",
        "    val_acc = accuracy_score(ys, ys_pred)\n",
        "    val_f1 = f1_score(ys, ys_pred, average='macro')\n",
        "    print(f\"[v2-fast] Epoch {epoch:02d}/{EPOCHS_V2} - loss: {train_loss:.4f} - val_acc: {val_acc:.4f} - val_macro_f1: {val_f1:.4f}\")\n",
        "\n",
        "    # Optional unfreeze backbone (skip in QUICK_MODE)\n",
        "    if (not unfrozen) and (not QUICK_MODE) and epoch >= 3:\n",
        "        for p in backbone_params:\n",
        "            p.requires_grad = True\n",
        "        optimizer_v2 = torch.optim.AdamW(model_v2.parameters(), lr=1e-3, weight_decay=WEIGHT_DECAY_V2)\n",
        "        scheduler_v2 = torch.optim.lr_scheduler.OneCycleLR(optimizer_v2, max_lr=1e-3, steps_per_epoch=steps_per_epoch,\n",
        "                                                           epochs=max(5, EPOCHS_V2-epoch), pct_start=0.2, div_factor=10.0, final_div_factor=10.0)\n",
        "        unfrozen = True\n",
        "        print('Backbone unfrozen and optimizer reset with lower LR')\n",
        "\n",
        "    if val_f1 > best_val_f1:\n",
        "        best_val_f1 = val_f1\n",
        "        torch.save({'model_state': model_v2.state_dict(),\n",
        "                    'config': {'img_size': IMG_SIZE_V2, 'num_classes': len(labels), 'channels': CHANNELS_V2,\n",
        "                               'mean': mean_v2, 'std': std_v2, 'backbone': model_v2.__class__.__name__}},\n",
        "                   model_path_v2)\n",
        "        print(f\"Saved best v2 model (val_macro_f1={best_val_f1:.4f}) -> {model_path_v2}\")\n",
        "        pat = 0\n",
        "    else:\n",
        "        pat += 1\n",
        "        if pat >= patience:\n",
        "            print('Early stopping triggered')\n",
        "            break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# v2-fast test evaluation\n",
        "if model_path_v2.exists():\n",
        "    ckpt = torch.load(model_path_v2, map_location=device)\n",
        "    model_v2.load_state_dict(ckpt['model_state'])\n",
        "\n",
        "model_v2.eval()\n",
        "ys, ys_pred = [], []\n",
        "with torch.no_grad():\n",
        "    for xb, yb in test_loader_v2_fast:\n",
        "        xb = xb.to(device)\n",
        "        yb = yb.to(device)\n",
        "        logits = model_v2(xb)\n",
        "        preds = logits.argmax(dim=1)\n",
        "        ys.extend(yb.cpu().numpy().tolist())\n",
        "        ys_pred.extend(preds.cpu().numpy().tolist())\n",
        "\n",
        "test_acc_v2 = accuracy_score(ys, ys_pred)\n",
        "test_f1_v2 = f1_score(ys, ys_pred, average='macro')\n",
        "print(f\"[v2-fast] Test accuracy: {test_acc_v2:.4f}, macro F1: {test_f1_v2:.4f}\")\n",
        "\n",
        "pd.DataFrame({\n",
        "    'path': list(X_test),\n",
        "    'true_label': [idx_to_label[int(i)] for i in ys],\n",
        "    'pred_label': [idx_to_label[int(i)] for i in ys_pred],\n",
        "}).to_csv(save_dir_v2 / 'test_predictions_v2_fast.csv', index=False)\n",
        "\n",
        "print('Saved v2-fast predictions to', save_dir_v2 / 'test_predictions_v2_fast.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inference helper v2\n",
        "class InferenceModelV2:\n",
        "    def __init__(self, model_path: Path, labels_path: Path):\n",
        "        with open(labels_path, 'r') as f:\n",
        "            self.idx_to_label = {int(k):v for k,v in json.load(f).items()}\n",
        "        ckpt = torch.load(model_path, map_location=device)\n",
        "        cfg = ckpt.get('config', {})\n",
        "        self.img_size = cfg.get('img_size', IMG_SIZE_V2)\n",
        "        self.channels = cfg.get('channels', CHANNELS_V2)\n",
        "        self.mean = cfg.get('mean', [0.5]*self.channels)\n",
        "        self.std = cfg.get('std', [0.5]*self.channels)\n",
        "        # Build model\n",
        "        try:\n",
        "            self.model = build_resnet18_head(len(self.idx_to_label), pretrained=False).to(device)\n",
        "        except Exception:\n",
        "            self.model = model_v2.__class__(len(self.idx_to_label)).to(device)\n",
        "        self.model.load_state_dict(ckpt['model_state'])\n",
        "        self.model.eval()\n",
        "        # Transforms\n",
        "        self.tfms = transforms.Compose([\n",
        "            transforms.Grayscale(num_output_channels=self.channels),\n",
        "            transforms.Resize((self.img_size, self.img_size)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(self.mean, self.std),\n",
        "        ])\n",
        "    @torch.no_grad()\n",
        "    def predict(self, image_path: Path):\n",
        "        with Image.open(image_path) as img:\n",
        "            img = img.convert('L')\n",
        "            x = self.tfms(img).unsqueeze(0).to(device)\n",
        "        logits = self.model(x)\n",
        "        pred = logits.argmax(dim=1).item()\n",
        "        return self.idx_to_label[pred]\n",
        "\n",
        "# Example:\n",
        "# infer_v2 = InferenceModelV2(model_path_v2, labels_path_v2)\n",
        "# infer_v2.predict(Path('Assets/...'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# (Removed) Baseline inference — use InferenceModelV2 with artifacts_v2\n",
        "'Use InferenceModelV2(model_path_v2, labels_path_v2)'\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
