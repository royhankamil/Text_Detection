{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "id": "ipTJBfNLbJiP"
      },
      "source": [
        "# Handwriting Image Classifier (PyTorch)\n",
        "\n",
        "This notebook trains a classifier for handwriting images using the datasets in `Assets/`:\n",
        "- `augmented_images/`: directory-of-directories (each subfolder is a class)\n",
        "- `handwritten-english-characters-and-digits/`: has `train/` and `test/` splits (each has class subfolders)\n",
        "- `image_labels.csv`: has `filename,label` mapping; we'll search for the files within `Assets/`\n",
        "\n",
        "Flow:\n",
        "- Discover and merge samples from all three sources\n",
        "- Build stratified train/val/test splits\n",
        "- Train a pretrained ResNet18-based model (with a CPU-friendly v2-fast option)\n",
        "- Evaluate and save the best model and label mapping\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OPTG77Qebgc8",
        "outputId": "803a96ca-2377-45ea-94f5-2ff29d974269"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hHpu6zjGbJi5",
        "outputId": "634d2634-03b8-4685-d93c-a9ed6452b27e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "pip install pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3UgQbkXBbJl7",
        "outputId": "1e000d6b-3773-4bda-a008-8af8b1851b81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU threads set to: 2\n"
          ]
        }
      ],
      "source": [
        "# System/perf settings\n",
        "import multiprocessing as mp\n",
        "from PIL import ImageFile\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "# Environment & imports\n",
        "import os, sys, json, random, math, time\n",
        "from pathlib import Path\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "# Cap CPU threads to avoid oversubscription\n",
        "try:\n",
        "    torch.set_num_threads(max(1, min(os.cpu_count() or 4, 8)))\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "print('CPU threads set to:', torch.get_num_threads())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HRAigSKEbJmC",
        "outputId": "5e25d7a9-2262-4c42-8bd1-6e0b6bd506ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "try:\n",
        "    import torchvision\n",
        "    from torchvision import transforms\n",
        "except Exception as e:\n",
        "    print(\"torchvision not found; attempting to continue with PIL-only transforms\")\n",
        "    torchvision = None\n",
        "    transforms = None\n",
        "\n",
        "ASSETS = Path('/content/drive/MyDrive/Asset-Handwritten-Detection/Assets')\n",
        "assert ASSETS.exists(), f\"Assets folder not found at {ASSETS.resolve()}\"\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WtLKgAU-bJmE",
        "outputId": "81730dc3-edb3-4312-ae17-30eaad809b54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "augmented_images: 13640 samples\n",
            "HED train: 2728 samples, HED test: 682 samples\n",
            "CSV samples resolved from image_labels.csv: 0 (missing: 13640)\n",
            "Total unique samples: 17050\n",
            "Num classes: 62\n",
            "Class distribution (top 20): [('0', 275), ('1', 275), ('2', 275), ('3', 275), ('4', 275), ('5', 275), ('6', 275), ('7', 275), ('8', 275), ('9', 275), ('A_caps', 275), ('B_caps', 275), ('C_caps', 275), ('D_caps', 275), ('E_caps', 275), ('F_caps', 275), ('G_caps', 275), ('H_caps', 275), ('I_caps', 275), ('J_caps', 275)]\n"
          ]
        }
      ],
      "source": [
        "# Discover datasets\n",
        "from typing import List, Tuple\n",
        "\n",
        "IMG_EXTS = {'.png', '.jpg', '.jpeg', '.bmp', '.gif'}\n",
        "\n",
        "def list_images_in_dir_of_dirs(root: Path) -> List[Tuple[Path, str]]:\n",
        "    samples = []\n",
        "    if not root.exists():\n",
        "        return samples\n",
        "    for class_dir in sorted([p for p in root.iterdir() if p.is_dir()]):\n",
        "        label = class_dir.name\n",
        "        for p in class_dir.rglob('*'):\n",
        "            if p.suffix.lower() in IMG_EXTS and p.is_file():\n",
        "                samples.append((p, label))\n",
        "    return samples\n",
        "\n",
        "# 1) augmented_images\n",
        "aug_dir = ASSETS / 'augmented_images'\n",
        "aug_samples = list_images_in_dir_of_dirs(aug_dir)\n",
        "print(f\"augmented_images: {len(aug_samples)} samples\")\n",
        "\n",
        "# 2) handwritten-english-characters-and-digits/train and test\n",
        "hed_root = ASSETS / 'handwritten-english-characters-and-digits'\n",
        "hed_train = list_images_in_dir_of_dirs(hed_root / 'train')\n",
        "hed_test = list_images_in_dir_of_dirs(hed_root / 'test')\n",
        "print(f\"HED train: {len(hed_train)} samples, HED test: {len(hed_test)} samples\")\n",
        "\n",
        "# 3) image_labels.csv: filename,label\n",
        "csv_path = ASSETS / 'image_labels.csv'\n",
        "if not csv_path.exists():\n",
        "    alt_csv_path = ASSETS / 'image_label.csv'  # support alternate name\n",
        "    if alt_csv_path.exists():\n",
        "        csv_path = alt_csv_path\n",
        "\n",
        "csv_samples = []\n",
        "if csv_path.exists():\n",
        "    df = pd.read_csv(csv_path)\n",
        "    assert {'filename','label'}.issubset(df.columns)\n",
        "    # Index all images under Assets for filename lookup\n",
        "    all_imgs = {p.name: p for p in ASSETS.rglob('*') if p.suffix.lower() in IMG_EXTS}\n",
        "    missing = 0\n",
        "    for _, row in df.iterrows():\n",
        "        fname = str(row['filename'])\n",
        "        label = str(row['label'])\n",
        "        if fname in all_imgs:\n",
        "            csv_samples.append((all_imgs[fname], label))\n",
        "        else:\n",
        "            # Try to search by suffix match if duplicates are unlikely\n",
        "            matches = [p for n,p in all_imgs.items() if n.endswith(fname)]\n",
        "            if len(matches) == 1:\n",
        "                csv_samples.append((matches[0], label))\n",
        "            else:\n",
        "                missing += 1\n",
        "    print(f\"CSV samples resolved from {csv_path.name}: {len(csv_samples)} (missing: {missing})\")\n",
        "else:\n",
        "    print(\"image_labels.csv not found; skipping CSV source\")\n",
        "\n",
        "# Merge all; if duplicates appear, prefer explicit CSV labels > hed > aug\n",
        "# Use image absolute path as key\n",
        "merged = {}\n",
        "for p,l in aug_samples:\n",
        "    merged[str(p.resolve())] = (p, l)\n",
        "for p,l in hed_train + hed_test:\n",
        "    merged[str(p.resolve())] = (p, l)\n",
        "for p,l in csv_samples:\n",
        "    merged[str(p.resolve())] = (p, l)\n",
        "\n",
        "all_samples = list(merged.values())\n",
        "print(f\"Total unique samples: {len(all_samples)}\")\n",
        "\n",
        "# Map labels to indices\n",
        "labels = sorted(sorted({l for _, l in all_samples}))\n",
        "label_to_idx = {l:i for i,l in enumerate(labels)}\n",
        "idx_to_label = {i:l for l,i in label_to_idx.items()}\n",
        "print(f\"Num classes: {len(labels)}\")\n",
        "\n",
        "# Class distribution\n",
        "counts = Counter([l for _,l in all_samples])\n",
        "print(\"Class distribution (top 20):\", counts.most_common(20))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-EJJfPhcbJmJ",
        "outputId": "5df7ca4b-3894-4e96-f55f-b142eb8b2365"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13640 1705 1705\n"
          ]
        }
      ],
      "source": [
        "# Train/Val/Test split (stratified)\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "random_seed = 42\n",
        "rng = np.random.RandomState(random_seed)\n",
        "\n",
        "paths = np.array([str(p) for p,_ in all_samples])\n",
        "labels_arr = np.array([label_to_idx[l] for _,l in all_samples])\n",
        "\n",
        "# If HED has an explicit test set, we already included it. We'll still split overall\n",
        "# into train/val/test=0.8/0.1/0.1 stratified.\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(\n",
        "    paths, labels_arr, test_size=0.1, random_state=random_seed, stratify=labels_arr\n",
        ")\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_temp, y_temp, test_size=0.1111, random_state=random_seed, stratify=y_temp\n",
        ")  # 0.8 * 0.1111 ≈ 0.0889 so final ≈ 80/10/10\n",
        "\n",
        "print(len(X_train), len(X_val), len(X_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uHtaOWGzbJmV"
      },
      "outputs": [],
      "source": [
        "# (Removed) Baseline transforms and dataset — superseded by v2. Keeping small helper for compatibility.\n",
        "IMG_SIZE = 64\n",
        "basic_train_tfms = None\n",
        "basic_val_tfms = None\n",
        "\n",
        "class HandwritingDataset(Dataset):\n",
        "    def __init__(self, paths: np.ndarray, labels: np.ndarray, transform=None):\n",
        "        self.paths = paths\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "    def __len__(self):\n",
        "        return len(self.paths)\n",
        "    def __getitem__(self, idx):\n",
        "        p = Path(self.paths[idx])\n",
        "        y = int(self.labels[idx])\n",
        "        with Image.open(p) as img:\n",
        "            img = img.convert('L')\n",
        "            if self.transform is not None:\n",
        "                x = self.transform(img)\n",
        "            else:\n",
        "                # minimal fallback\n",
        "                img = img.resize((IMG_SIZE, IMG_SIZE))\n",
        "                arr = np.array(img, dtype=np.float32)/255.0\n",
        "                arr = arr[None, ...]\n",
        "                x = torch.from_numpy(arr)\n",
        "        return x, y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "yrCmNChlbJmY",
        "outputId": "d637f01c-5666-4c48-dfd7-3a84329c619a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Use cells 17–19 for training/eval loaders.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "# (Removed) Baseline dataloaders — use v2 or v2-fast loaders below\n",
        "'Use cells 17–19 for training/eval loaders.'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "oHwjkfR2bJnK",
        "outputId": "dd1e08eb-a502-4eef-a5ca-e48b70c47312"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Use cells 11–19 for model, training and evaluation.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# (Removed) SmallCNN baseline — using pretrained ResNet18 (v2)\n",
        "'Use cells 11–19 for model, training and evaluation.'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "e9xELc_ybJnq",
        "outputId": "f4d8a744-5d61-4728-8680-62a2d6b581aa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Use cells 18–19 (or 15–16) for training and evaluation.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "# (Removed) Baseline training utilities — replaced by v2 training below\n",
        "'Use cells 18–19 (or 15–16) for training and evaluation.'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "h4KqntfLbJns",
        "outputId": "a7cc4382-1af7-40c2-c06d-21595421e88c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Use the v2 model: artifacts_v2/handwriting_resnet18_best.pt'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "# (Removed) Baseline evaluation — use v2/v2-fast evaluation cells (16 or 19)\n",
        "'Use the v2 model: artifacts_v2/handwriting_resnet18_best.pt'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t4M8UPWsbJoA"
      },
      "outputs": [],
      "source": [
        "# Improved configuration (v2)\n",
        "IMG_SIZE_V2 = 128\n",
        "CHANNELS_V2 = 3  # use 3-channel to leverage pretrained backbones\n",
        "EPOCHS_V2 = 30\n",
        "BATCH_SIZE_V2 = 128\n",
        "WEIGHT_DECAY_V2 = 1e-4\n",
        "LABEL_SMOOTHING_V2 = 0.1\n",
        "RANDOM_SEED_V2 = 42\n",
        "USE_PRETRAINED_V2 = True  # will fallback automatically if not available\n",
        "\n",
        "\n",
        "# Quick mode for CPU/fast runs\n",
        "QUICK_MODE = True if device.type == 'cpu' else False\n",
        "# Override defaults when quick mode is on\n",
        "if QUICK_MODE:\n",
        "    IMG_SIZE_V2 = 96\n",
        "    EPOCHS_V2 = 5\n",
        "    BATCH_SIZE_V2 = 32\n",
        "USE_WEIGHTED_SAMPLER = not QUICK_MODE\n",
        "FREEZE_BACKBONE_ONLY = QUICK_MODE  # keep backbone frozen for speed in quick mode\n",
        "\n",
        "\n",
        "rng_v2 = np.random.RandomState(RANDOM_SEED_V2)\n",
        "\n",
        "# Create artifacts dir for v2\n",
        "save_dir_v2 = Path('/content/drive/MyDrive/Asset-Handwritten-Detection/result')\n",
        "save_dir_v2.mkdir(exist_ok=True, parents=True)\n",
        "model_path_v2 = save_dir_v2 / 'handwriting_resnet18_best.pt'\n",
        "labels_path_v2 = save_dir_v2 / 'labels.json'\n",
        "with open(labels_path_v2, 'w') as f:\n",
        "    json.dump(idx_to_label, f, indent=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-vkDzYqbJoB",
        "outputId": "7ab83a07-98d1-4776-da3b-c3c250e49e7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 97.0MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using ResNet18 backbone for v2. Imagenet norm: True\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11208318, 'ResNet')"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "# Model v2: ResNet18 backbone with pretrained fallback\n",
        "from typing import Optional\n",
        "\n",
        "USE_IMAGENET_NORM_V2 = False\n",
        "\n",
        "def build_resnet18_head(num_classes: int, pretrained: bool = True) -> nn.Module:\n",
        "    global USE_IMAGENET_NORM_V2\n",
        "    if torchvision is None:\n",
        "        raise RuntimeError('torchvision not available')\n",
        "    try:\n",
        "        try:\n",
        "            # torchvision >=0.13\n",
        "            from torchvision.models import resnet18, ResNet18_Weights\n",
        "            weights = ResNet18_Weights.IMAGENET1K_V1 if pretrained else None\n",
        "            model = resnet18(weights=weights)\n",
        "            USE_IMAGENET_NORM_V2 = pretrained\n",
        "        except Exception:\n",
        "            # older API\n",
        "            from torchvision.models import resnet18\n",
        "            model = resnet18(pretrained=pretrained)\n",
        "            USE_IMAGENET_NORM_V2 = pretrained\n",
        "    except Exception as e:\n",
        "        print('Pretrained resnet18 unavailable, falling back to randomly initialized.')\n",
        "        from torchvision.models import resnet18\n",
        "        model = resnet18(pretrained=False)\n",
        "        USE_IMAGENET_NORM_V2 = False\n",
        "    # Adjust first layer to accept 3-channel grayscale (we feed 3-channel grayscale, so no change needed)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, num_classes)\n",
        "    return model\n",
        "\n",
        "try:\n",
        "    model_v2 = build_resnet18_head(len(labels), pretrained=USE_PRETRAINED_V2).to(device)\n",
        "    print('Using ResNet18 backbone for v2. Imagenet norm:', USE_IMAGENET_NORM_V2)\n",
        "except Exception as e:\n",
        "    print('Falling back to SmallCNN due to error:', e)\n",
        "    CHANNELS_V2 = 1\n",
        "    class DeeperCNN(nn.Module):\n",
        "        def __init__(self, num_classes: int):\n",
        "            super().__init__()\n",
        "            self.features = nn.Sequential(\n",
        "                nn.Conv2d(1, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(),\n",
        "                nn.Conv2d(64, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(),\n",
        "                nn.MaxPool2d(2),\n",
        "                nn.Conv2d(64, 128, 3, padding=1), nn.BatchNorm2d(128), nn.ReLU(),\n",
        "                nn.Conv2d(128, 128, 3, padding=1), nn.BatchNorm2d(128), nn.ReLU(),\n",
        "                nn.MaxPool2d(2),\n",
        "                nn.Conv2d(128, 256, 3, padding=1), nn.BatchNorm2d(256), nn.ReLU(),\n",
        "                nn.MaxPool2d(2),\n",
        "            )\n",
        "            fsize = IMG_SIZE_V2 // 8\n",
        "            self.classifier = nn.Sequential(\n",
        "                nn.Dropout(0.4), nn.Linear(256*fsize*fsize, 512), nn.ReLU(),\n",
        "                nn.Dropout(0.4), nn.Linear(512, num_classes)\n",
        "            )\n",
        "        def forward(self, x):\n",
        "            x = self.features(x)\n",
        "            x = torch.flatten(x, 1)\n",
        "            return self.classifier(x)\n",
        "    model_v2 = DeeperCNN(len(labels)).to(device)\n",
        "    USE_IMAGENET_NORM_V2 = False\n",
        "\n",
        "sum(p.numel() for p in model_v2.parameters() if p.requires_grad), model_v2.__class__.__name__\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q-RUGBT5bJqW"
      },
      "outputs": [],
      "source": [
        "# Transforms v2 (with optional dataset normalization)\n",
        "from typing import Tuple\n",
        "\n",
        "def compute_mean_std(paths_subset: np.ndarray, channels: int, sample_size: int = 1024) -> Tuple[list, list]:\n",
        "    sample_paths = paths_subset.copy()\n",
        "    if len(sample_paths) > sample_size:\n",
        "        sample_paths = rng_v2.choice(sample_paths, size=sample_size, replace=False)\n",
        "    means = np.zeros(channels, dtype=np.float64)\n",
        "    sq_means = np.zeros(channels, dtype=np.float64)\n",
        "    n_pix_total = 0\n",
        "    for p in sample_paths:\n",
        "        with Image.open(p) as img:\n",
        "            img = img.convert('L')  # grayscale base\n",
        "            img = img.resize((IMG_SIZE_V2, IMG_SIZE_V2))\n",
        "            arr = np.array(img, dtype=np.float32) / 255.0\n",
        "            if channels == 1:\n",
        "                arr_c = arr[None, ...]\n",
        "            else:\n",
        "                arr_c = np.stack([arr, arr, arr], axis=0)\n",
        "            n_pix = arr_c.shape[1] * arr_c.shape[2]\n",
        "            means += arr_c.reshape(channels, -1).sum(axis=1)\n",
        "            sq_means += (arr_c.reshape(channels, -1) ** 2).sum(axis=1)\n",
        "            n_pix_total += n_pix\n",
        "    means /= n_pix_total\n",
        "    stds = np.sqrt(sq_means / n_pix_total - means**2)\n",
        "    return means.tolist(), stds.tolist()\n",
        "\n",
        "if transforms is None:\n",
        "    raise RuntimeError('torchvision is required for v2 pipeline transforms')\n",
        "\n",
        "if USE_IMAGENET_NORM_V2:\n",
        "    # ImageNet mean/std\n",
        "    mean_v2 = [0.485, 0.456, 0.406]\n",
        "    std_v2 = [0.229, 0.224, 0.225]\n",
        "else:\n",
        "    mean_v2, std_v2 = compute_mean_std(X_train, CHANNELS_V2, sample_size=1024)\n",
        "\n",
        "train_tfms_v2 = transforms.Compose([\n",
        "    transforms.Grayscale(num_output_channels=CHANNELS_V2),\n",
        "    transforms.Resize(int(IMG_SIZE_V2*1.1)),\n",
        "    transforms.RandomCrop(IMG_SIZE_V2),\n",
        "    transforms.RandomRotation(8, fill=0),\n",
        "    transforms.RandomAffine(degrees=0, shear=8, translate=(0.05,0.05)),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
        "    transforms.RandomPerspective(distortion_scale=0.2, p=0.3),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean_v2, std_v2),\n",
        "])\n",
        "\n",
        "val_tfms_v2 = transforms.Compose([\n",
        "    transforms.Grayscale(num_output_channels=CHANNELS_V2),\n",
        "    transforms.Resize((IMG_SIZE_V2, IMG_SIZE_V2)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean_v2, std_v2),\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hfTwzHrZbJqX",
        "outputId": "a82402d3-1724-44f6-f7f1-c0f38a400d48"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(13640,\n",
              " 1705,\n",
              " 1705,\n",
              " [(34, 220),\n",
              "  (42, 220),\n",
              "  (52, 220),\n",
              "  (53, 220),\n",
              "  (37, 220),\n",
              "  (47, 220),\n",
              "  (50, 220),\n",
              "  (17, 220),\n",
              "  (55, 220),\n",
              "  (51, 220)])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "# Dataset & DataLoaders v2 with optional class balancing\n",
        "from torch.utils.data import WeightedRandomSampler\n",
        "\n",
        "class HandwritingDatasetV2(Dataset):\n",
        "    def __init__(self, paths: np.ndarray, labels: np.ndarray, transform=None):\n",
        "        self.paths = paths\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "    def __len__(self):\n",
        "        return len(self.paths)\n",
        "    def __getitem__(self, idx):\n",
        "        p = Path(self.paths[idx])\n",
        "        y = int(self.labels[idx])\n",
        "        with Image.open(p) as img:\n",
        "            img = img.convert('L')\n",
        "            x = (self.transform or val_tfms_v2)(img)\n",
        "        return x, y\n",
        "\n",
        "train_ds_v2 = HandwritingDatasetV2(X_train, y_train, transform=train_tfms_v2)\n",
        "val_ds_v2   = HandwritingDatasetV2(X_val, y_val, transform=val_tfms_v2)\n",
        "test_ds_v2  = HandwritingDatasetV2(X_test, y_test, transform=val_tfms_v2)\n",
        "\n",
        "# Class weights for sampler and loss\n",
        "class_counts = Counter(y_train.tolist())\n",
        "num_classes = len(labels)\n",
        "class_freq = np.array([class_counts.get(i, 0) for i in range(num_classes)], dtype=np.float64)\n",
        "class_weights = 1.0 / np.maximum(class_freq, 1.0)\n",
        "class_weights = class_weights / class_weights.sum() * num_classes\n",
        "\n",
        "# DataLoaders: optionally use weighted sampler\n",
        "num_workers_v2 = 0 if os.name == 'nt' else 2\n",
        "if USE_WEIGHTED_SAMPLER:\n",
        "    sample_weights = np.array([class_weights[y] for y in y_train], dtype=np.float64)\n",
        "    sampler = WeightedRandomSampler(weights=torch.DoubleTensor(sample_weights), num_samples=len(sample_weights), replacement=True)\n",
        "    train_loader_v2 = DataLoader(train_ds_v2, batch_size=BATCH_SIZE_V2, sampler=sampler, num_workers=num_workers_v2, pin_memory=(device.type=='cuda'))\n",
        "else:\n",
        "    train_loader_v2 = DataLoader(train_ds_v2, batch_size=BATCH_SIZE_V2, shuffle=True, num_workers=num_workers_v2, pin_memory=(device.type=='cuda'))\n",
        "val_loader_v2   = DataLoader(val_ds_v2, batch_size=BATCH_SIZE_V2, shuffle=False, num_workers=num_workers_v2)\n",
        "test_loader_v2  = DataLoader(test_ds_v2, batch_size=BATCH_SIZE_V2, shuffle=False, num_workers=num_workers_v2)\n",
        "\n",
        "len(train_ds_v2), len(val_ds_v2), len(test_ds_v2), class_counts.most_common(10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q8_4X7f1bJt_",
        "outputId": "f0ef7dd1-836c-4b7e-eaf8-1246605ccdd4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2646473394.py:12: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler(enabled=(device.type=='cuda'))\n",
            "/tmp/ipython-input-2646473394.py:28: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=(device.type=='cuda')):\n",
            "/tmp/ipython-input-2646473394.py:46: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[v2] Epoch 01/5 - loss: 2.2610 - val_acc: 0.5091 - val_macro_f1: 0.4921\n",
            "Saved best v2 model (val_macro_f1=0.4921) -> /content/drive/MyDrive/Asset-Handwritten-Detection/result/handwriting_resnet18_best.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2646473394.py:28: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=(device.type=='cuda')):\n",
            "/tmp/ipython-input-2646473394.py:46: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[v2] Epoch 02/5 - loss: 1.5682 - val_acc: 0.7865 - val_macro_f1: 0.7750\n",
            "Saved best v2 model (val_macro_f1=0.7750) -> /content/drive/MyDrive/Asset-Handwritten-Detection/result/handwriting_resnet18_best.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2646473394.py:28: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=(device.type=='cuda')):\n",
            "/tmp/ipython-input-2646473394.py:46: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[v2] Epoch 03/5 - loss: 1.2693 - val_acc: 0.8346 - val_macro_f1: 0.8359\n",
            "Saved best v2 model (val_macro_f1=0.8359) -> /content/drive/MyDrive/Asset-Handwritten-Detection/result/handwriting_resnet18_best.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2646473394.py:28: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=(device.type=='cuda')):\n",
            "/tmp/ipython-input-2646473394.py:46: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[v2] Epoch 04/5 - loss: 1.0968 - val_acc: 0.8663 - val_macro_f1: 0.8666\n",
            "Saved best v2 model (val_macro_f1=0.8666) -> /content/drive/MyDrive/Asset-Handwritten-Detection/result/handwriting_resnet18_best.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2646473394.py:28: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=(device.type=='cuda')):\n",
            "/tmp/ipython-input-2646473394.py:46: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[v2] Epoch 05/5 - loss: 0.9930 - val_acc: 0.8768 - val_macro_f1: 0.8764\n",
            "Saved best v2 model (val_macro_f1=0.8764) -> /content/drive/MyDrive/Asset-Handwritten-Detection/result/handwriting_resnet18_best.pt\n",
            "Best v2 val macro F1: 0.8764436068015303\n"
          ]
        }
      ],
      "source": [
        "# Training loop v2 with AMP, label smoothing, AdamW, OneCycleLR, early stopping on macro F1\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "criterion_v2 = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32, device=device),\n",
        "                                   label_smoothing=LABEL_SMOOTHING_V2)\n",
        "optimizer_v2 = torch.optim.AdamW(model_v2.parameters(), lr=3e-3, weight_decay=WEIGHT_DECAY_V2)\n",
        "steps_per_epoch = max(1, math.ceil(len(train_ds_v2) / BATCH_SIZE_V2))\n",
        "scheduler_v2 = torch.optim.lr_scheduler.OneCycleLR(optimizer_v2, max_lr=3e-3, steps_per_epoch=steps_per_epoch,\n",
        "                                                   epochs=EPOCHS_V2, pct_start=0.2, div_factor=10.0, final_div_factor=10.0)\n",
        "\n",
        "scaler = GradScaler(enabled=(device.type=='cuda'))\n",
        "\n",
        "best_val_f1 = 0.0\n",
        "patience = 6\n",
        "pat = 0\n",
        "\n",
        "history = {'epoch': [], 'train_loss': [], 'val_acc': [], 'val_f1': []}\n",
        "\n",
        "for epoch in range(1, EPOCHS_V2+1):\n",
        "    model_v2.train()\n",
        "    running_loss = 0.0\n",
        "    b = 0\n",
        "    for xb, yb in train_loader_v2:\n",
        "        xb = xb.to(device, non_blocking=True)\n",
        "        yb = yb.to(device, non_blocking=True)\n",
        "        optimizer_v2.zero_grad(set_to_none=True)\n",
        "        with autocast(enabled=(device.type=='cuda')):\n",
        "            logits = model_v2(xb)\n",
        "            loss = criterion_v2(logits, yb)\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer_v2)\n",
        "        scaler.update()\n",
        "        scheduler_v2.step()\n",
        "        running_loss += loss.item()\n",
        "        b += 1\n",
        "    train_loss = running_loss / max(1, b)\n",
        "\n",
        "    # Validate\n",
        "    model_v2.eval()\n",
        "    ys, ys_pred = [], []\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in val_loader_v2:\n",
        "            xb = xb.to(device)\n",
        "            yb = yb.to(device)\n",
        "            with autocast(enabled=(device.type=='cuda')):\n",
        "                logits = model_v2(xb)\n",
        "            preds = logits.argmax(dim=1)\n",
        "            ys.extend(yb.cpu().numpy().tolist())\n",
        "            ys_pred.extend(preds.cpu().numpy().tolist())\n",
        "    val_acc = accuracy_score(ys, ys_pred)\n",
        "    val_f1 = f1_score(ys, ys_pred, average='macro')\n",
        "    history['epoch'].append(epoch)\n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['val_acc'].append(val_acc)\n",
        "    history['val_f1'].append(val_f1)\n",
        "\n",
        "    print(f\"[v2] Epoch {epoch:02d}/{EPOCHS_V2} - loss: {train_loss:.4f} - val_acc: {val_acc:.4f} - val_macro_f1: {val_f1:.4f}\")\n",
        "\n",
        "    if val_f1 > best_val_f1:\n",
        "        best_val_f1 = val_f1\n",
        "        torch.save({'model_state': model_v2.state_dict(),\n",
        "                    'config': {'img_size': IMG_SIZE_V2, 'num_classes': len(labels), 'channels': CHANNELS_V2,\n",
        "                               'mean': mean_v2, 'std': std_v2, 'backbone': model_v2.__class__.__name__}},\n",
        "                   model_path_v2)\n",
        "        print(f\"Saved best v2 model (val_macro_f1={best_val_f1:.4f}) -> {model_path_v2}\")\n",
        "        pat = 0\n",
        "    else:\n",
        "        pat += 1\n",
        "        if pat >= patience:\n",
        "            print('Early stopping triggered')\n",
        "            break\n",
        "\n",
        "json.dump(history, open(save_dir_v2 / 'train_history.json', 'w'), indent=2)\n",
        "print('Best v2 val macro F1:', best_val_f1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "71kyDghPbJuC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "760a68ac-723b-435d-e1f3-2c1a48e05928"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[v2] Test accuracy: 0.8774, macro F1: 0.8748\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.40      1.00      0.57        27\n",
            "           1       0.69      0.79      0.73        28\n",
            "           2       0.87      0.93      0.90        28\n",
            "           3       1.00      1.00      1.00        27\n",
            "           4       1.00      1.00      1.00        27\n",
            "           5       0.96      0.93      0.94        27\n",
            "           6       0.86      0.89      0.87        27\n",
            "           7       1.00      0.89      0.94        28\n",
            "           8       1.00      0.96      0.98        27\n",
            "           9       0.88      0.81      0.85        27\n",
            "      A_caps       0.97      1.00      0.98        28\n",
            "      B_caps       1.00      0.89      0.94        28\n",
            "      C_caps       0.88      0.52      0.65        27\n",
            "      D_caps       1.00      0.96      0.98        27\n",
            "      E_caps       1.00      0.93      0.96        28\n",
            "      F_caps       1.00      0.93      0.96        27\n",
            "      G_caps       1.00      0.89      0.94        27\n",
            "      H_caps       1.00      1.00      1.00        27\n",
            "      I_caps       0.71      0.96      0.82        28\n",
            "      J_caps       0.90      1.00      0.95        27\n",
            "      K_caps       0.96      0.79      0.86        28\n",
            "      L_caps       0.97      1.00      0.98        28\n",
            "      M_caps       1.00      0.96      0.98        28\n",
            "      N_caps       1.00      1.00      1.00        28\n",
            "      O_caps       1.00      0.14      0.25        28\n",
            "      P_caps       0.95      0.74      0.83        27\n",
            "      Q_caps       1.00      0.89      0.94        28\n",
            "      R_caps       1.00      1.00      1.00        28\n",
            "      S_caps       0.75      0.64      0.69        28\n",
            "      T_caps       0.97      1.00      0.98        28\n",
            "      U_caps       1.00      1.00      1.00        27\n",
            "      V_caps       0.87      0.74      0.80        27\n",
            "      W_caps       0.92      0.79      0.85        28\n",
            "      X_caps       1.00      0.89      0.94        27\n",
            "      Y_caps       0.81      0.96      0.88        27\n",
            "      Z_caps       1.00      0.68      0.81        28\n",
            "           a       0.89      0.93      0.91        27\n",
            "           b       0.93      0.96      0.95        28\n",
            "           c       0.59      0.93      0.72        28\n",
            "           d       1.00      0.96      0.98        28\n",
            "           e       1.00      0.86      0.92        28\n",
            "           f       0.93      0.96      0.95        27\n",
            "           g       0.90      0.93      0.91        28\n",
            "           h       0.97      1.00      0.98        28\n",
            "           i       0.93      1.00      0.97        28\n",
            "           j       1.00      0.93      0.96        27\n",
            "           k       0.82      0.96      0.89        28\n",
            "           l       0.82      0.67      0.73        27\n",
            "           m       1.00      0.63      0.77        27\n",
            "           n       0.74      1.00      0.85        28\n",
            "           o       0.80      0.30      0.43        27\n",
            "           p       0.75      0.96      0.84        28\n",
            "           q       0.89      0.89      0.89        27\n",
            "           r       1.00      0.96      0.98        27\n",
            "           s       0.55      0.78      0.65        27\n",
            "           t       1.00      1.00      1.00        28\n",
            "           u       1.00      1.00      1.00        27\n",
            "           v       0.73      0.89      0.80        27\n",
            "           w       0.79      0.85      0.82        27\n",
            "           x       0.90      0.96      0.93        27\n",
            "           y       0.91      0.75      0.82        28\n",
            "           z       0.77      0.82      0.79        28\n",
            "\n",
            "    accuracy                           0.88      1705\n",
            "   macro avg       0.90      0.88      0.87      1705\n",
            "weighted avg       0.90      0.88      0.87      1705\n",
            "\n",
            "Confusion matrix shape: (62, 62)\n",
            "Saved v2 predictions to /content/drive/MyDrive/Asset-Handwritten-Detection/result/test_predictions_v2.csv\n"
          ]
        }
      ],
      "source": [
        "# Test evaluation v2\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "if model_path_v2.exists():\n",
        "    ckpt = torch.load(model_path_v2, map_location=device)\n",
        "    model_v2.load_state_dict(ckpt['model_state'])\n",
        "\n",
        "model_v2.eval()\n",
        "ys, ys_pred = [], []\n",
        "with torch.no_grad():\n",
        "    for xb, yb in test_loader_v2:\n",
        "        xb = xb.to(device)\n",
        "        yb = yb.to(device)\n",
        "        logits = model_v2(xb)\n",
        "        preds = logits.argmax(dim=1)\n",
        "        ys.extend(yb.cpu().numpy().tolist())\n",
        "        ys_pred.extend(preds.cpu().numpy().tolist())\n",
        "\n",
        "test_acc_v2 = accuracy_score(ys, ys_pred)\n",
        "test_f1_v2 = f1_score(ys, ys_pred, average='macro')\n",
        "print(f\"[v2] Test accuracy: {test_acc_v2:.4f}, macro F1: {test_f1_v2:.4f}\")\n",
        "\n",
        "report_v2 = classification_report(ys, ys_pred, target_names=[idx_to_label[i] for i in range(len(labels))])\n",
        "print(report_v2)\n",
        "\n",
        "cm_v2 = confusion_matrix(ys, ys_pred)\n",
        "print('Confusion matrix shape:', cm_v2.shape)\n",
        "\n",
        "pd.DataFrame({\n",
        "    'path': list(X_test),\n",
        "    'true_label': [idx_to_label[int(i)] for i in ys],\n",
        "    'pred_label': [idx_to_label[int(i)] for i in ys_pred],\n",
        "}).to_csv(save_dir_v2 / 'test_predictions_v2.csv', index=False)\n",
        "\n",
        "print('Saved v2 predictions to', save_dir_v2 / 'test_predictions_v2.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "418SsBWjbJv0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0099b18d-d4fa-4cef-8681-36b5c3e360bd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(13640, 1705, 1705, 1, False)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "# Fast(er) loaders for CPU, and lighter transforms if no CUDA\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "pin_memory_v2 = (device.type == 'cuda')\n",
        "num_workers_v2 = 0 if os.name == 'nt' else max(1, min(4, (os.cpu_count() or 2) - 1))\n",
        "\n",
        "if transforms is None:\n",
        "    raise RuntimeError('torchvision is required for v2 fast pipeline')\n",
        "\n",
        "if device.type == 'cuda':\n",
        "    train_tfms_v2_fast = train_tfms_v2\n",
        "    val_tfms_v2_fast = val_tfms_v2\n",
        "else:\n",
        "    # Lighter CPU transforms\n",
        "    train_tfms_v2_fast = transforms.Compose([\n",
        "        transforms.Grayscale(num_output_channels=CHANNELS_V2),\n",
        "        transforms.Resize(int(IMG_SIZE_V2*1.05)),\n",
        "        transforms.RandomCrop(IMG_SIZE_V2),\n",
        "        transforms.RandomRotation(6, fill=0),\n",
        "        transforms.RandomAffine(degrees=0, shear=6, translate=(0.03,0.03)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean_v2, std_v2),\n",
        "    ])\n",
        "    val_tfms_v2_fast = transforms.Compose([\n",
        "        transforms.Grayscale(num_output_channels=CHANNELS_V2),\n",
        "        transforms.Resize((IMG_SIZE_V2, IMG_SIZE_V2)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean_v2, std_v2),\n",
        "    ])\n",
        "\n",
        "train_ds_v2_fast = HandwritingDatasetV2(X_train, y_train, transform=train_tfms_v2_fast)\n",
        "val_ds_v2_fast   = HandwritingDatasetV2(X_val, y_val, transform=val_tfms_v2_fast)\n",
        "test_ds_v2_fast  = HandwritingDatasetV2(X_test, y_test, transform=val_tfms_v2_fast)\n",
        "\n",
        "# Rebuild sampler to match dataset length\n",
        "class_counts = Counter(y_train.tolist())\n",
        "num_classes = len(labels)\n",
        "class_freq = np.array([class_counts.get(i, 0) for i in range(num_classes)], dtype=np.float64)\n",
        "class_weights = 1.0 / np.maximum(class_freq, 1.0)\n",
        "class_weights = class_weights / class_weights.sum() * num_classes\n",
        "sample_weights = np.array([class_weights[y] for y in y_train], dtype=np.float64)\n",
        "sampler_fast = WeightedRandomSampler(weights=torch.DoubleTensor(sample_weights), num_samples=len(sample_weights), replacement=True)\n",
        "\n",
        "train_loader_v2_fast = DataLoader(train_ds_v2_fast, batch_size=BATCH_SIZE_V2, sampler=sampler_fast, num_workers=num_workers_v2, pin_memory=pin_memory_v2)\n",
        "val_loader_v2_fast   = DataLoader(val_ds_v2_fast, batch_size=BATCH_SIZE_V2, shuffle=False, num_workers=num_workers_v2, pin_memory=pin_memory_v2)\n",
        "test_loader_v2_fast  = DataLoader(test_ds_v2_fast, batch_size=BATCH_SIZE_V2, shuffle=False, num_workers=num_workers_v2, pin_memory=pin_memory_v2)\n",
        "\n",
        "len(train_ds_v2_fast), len(val_ds_v2_fast), len(test_ds_v2_fast), num_workers_v2, pin_memory_v2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CYhEgnCPbJv1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c9366cc-b5ba-4791-bd9f-8312db15881b"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "The size of tensor a (512) must match the size of tensor b (7) at non-singleton dimension 3",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-864051614.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Head-only warmup: train final layer(s) first\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mhead_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_v2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_v2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fc'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_v2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mbackbone_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel_v2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhead_params\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbackbone_params\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mQUICK_MODE\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-864051614.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Head-only warmup: train final layer(s) first\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mhead_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_v2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_v2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fc'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_v2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mbackbone_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel_v2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhead_params\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbackbone_params\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mQUICK_MODE\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (512) must match the size of tensor b (7) at non-singleton dimension 3"
          ]
        }
      ],
      "source": [
        "# v2 training (fast) with head-only warmup then partial unfreeze, tqdm, new torch.amp API\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "# Update AMP API per warning\n",
        "use_amp = (device.type == 'cuda')\n",
        "scaler = torch.amp.GradScaler(device.type) if use_amp else None\n",
        "\n",
        "# Rebuild optimizer & scheduler\n",
        "for p in model_v2.parameters():\n",
        "    p.requires_grad = True\n",
        "\n",
        "# Head-only warmup: train final layer(s) first\n",
        "head_params = list(model_v2.fc.parameters()) if hasattr(model_v2, 'fc') else list(model_v2.classifier.parameters())\n",
        "backbone_params = [p for p in model_v2.parameters() if p not in head_params]\n",
        "for p in backbone_params:\n",
        "    p.requires_grad = False if QUICK_MODE else True\n",
        "\n",
        "optimizer_v2 = torch.optim.AdamW(filter(lambda p: p.requires_grad, model_v2.parameters()), lr=3e-3, weight_decay=WEIGHT_DECAY_V2)\n",
        "steps_per_epoch = max(1, math.ceil(len(train_ds_v2_fast) / BATCH_SIZE_V2))\n",
        "scheduler_v2 = torch.optim.lr_scheduler.OneCycleLR(optimizer_v2, max_lr=3e-3, steps_per_epoch=steps_per_epoch,\n",
        "                                                   epochs=EPOCHS_V2, pct_start=0.2, div_factor=10.0, final_div_factor=10.0)\n",
        "\n",
        "criterion_v2 = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32, device=device),\n",
        "                                   label_smoothing=LABEL_SMOOTHING_V2)\n",
        "\n",
        "best_val_f1 = 0.0\n",
        "patience = 3 if QUICK_MODE else 5\n",
        "pat = 0\n",
        "\n",
        "unfrozen = False\n",
        "\n",
        "# Create the result directory\n",
        "result_dir = Path('/content/drive/MyDrive/Asset-Handwritten-Detection/result')\n",
        "result_dir.mkdir(parents=True, exist_ok=True)\n",
        "model_path_v2_result = result_dir / 'handwriting_resnet18_best.pt'\n",
        "labels_path_v2_result = result_dir / 'labels.json'\n",
        "with open(labels_path_v2_result, 'w') as f:\n",
        "    json.dump(idx_to_label, f, indent=2)\n",
        "\n",
        "for epoch in range(1, EPOCHS_V2+1):\n",
        "    model_v2.train()\n",
        "    running_loss = 0.0\n",
        "    pbar = tqdm(train_loader_v2_fast, desc=f\"[v2-fast] Epoch {epoch}/{EPOCHS_V2}\")\n",
        "    for xb, yb in pbar:\n",
        "        xb = xb.to(device)\n",
        "        yb = yb.to(device)\n",
        "        optimizer_v2.zero_grad(set_to_none=True)\n",
        "        if use_amp:\n",
        "            with torch.amp.autocast(device_type=device.type, enabled=True):\n",
        "                logits = model_v2(xb)\n",
        "                loss = criterion_v2(logits, yb)\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer_v2)\n",
        "            scaler.update()\n",
        "        else:\n",
        "            logits = model_v2(xb)\n",
        "            loss = criterion_v2(logits, yb)\n",
        "            loss.backward()\n",
        "            optimizer_v2.step()\n",
        "        scheduler_v2.step()\n",
        "        running_loss += loss.item()\n",
        "        pbar.set_postfix({'loss': f\"{running_loss / (pbar.n or 1):.4f}\"})\n",
        "    train_loss = running_loss / max(1, len(train_loader_v2_fast))\n",
        "\n",
        "    # Validate\n",
        "    model_v2.eval()\n",
        "    ys, ys_pred = [], []\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in val_loader_v2_fast:\n",
        "            xb = xb.to(device)\n",
        "            yb = yb.to(device)\n",
        "            if use_amp:\n",
        "                with torch.amp.autocast(device_type=device.type, enabled=True):\n",
        "                    logits = model_v2(xb)\n",
        "            else:\n",
        "                logits = model_v2(xb)\n",
        "            preds = logits.argmax(dim=1)\n",
        "            ys.extend(yb.cpu().numpy().tolist())\n",
        "            ys_pred.extend(preds.cpu().numpy().tolist())\n",
        "    val_acc = accuracy_score(ys, ys_pred)\n",
        "    val_f1 = f1_score(ys, ys_pred, average='macro')\n",
        "    print(f\"[v2-fast] Epoch {epoch:02d}/{EPOCHS_V2} - loss: {train_loss:.4f} - val_acc: {val_acc:.4f} - val_macro_f1: {val_f1:.4f}\")\n",
        "\n",
        "    # Optional unfreeze backbone (skip in QUICK_MODE)\n",
        "    if (not unfrozen) and (not QUICK_MODE) and epoch >= 3:\n",
        "        for p in backbone_params:\n",
        "            p.requires_grad = True\n",
        "        optimizer_v2 = torch.optim.AdamW(model_v2.parameters(), lr=1e-3, weight_decay=WEIGHT_DECAY_V2)\n",
        "        scheduler_v2 = torch.optim.lr_scheduler.OneCycleLR(optimizer_v2, max_lr=1e-3, steps_per_epoch=steps_per_epoch,\n",
        "                                                           epochs=max(5, EPOCHS_V2-epoch), pct_start=0.2, div_factor=10.0, final_div_factor=10.0)\n",
        "        unfrozen = True\n",
        "        print('Backbone unfrozen and optimizer reset with lower LR')\n",
        "\n",
        "    if val_f1 > best_val_f1:\n",
        "        best_val_f1 = val_f1\n",
        "        torch.save({'model_state': model_v2.state_dict(),\n",
        "                    'config': {'img_size': IMG_SIZE_V2, 'num_classes': len(labels), 'channels': CHANNELS_V2,\n",
        "                               'mean': mean_v2, 'std': std_v2, 'backbone': model_v2.__class__.__name__}},\n",
        "                   model_path_v2_result)\n",
        "        print(f\"Saved best v2 model (val_macro_f1={best_val_f1:.4f}) -> {model_path_v2_result}\")\n",
        "        pat = 0\n",
        "    else:\n",
        "        pat += 1\n",
        "        if pat >= patience:\n",
        "            print('Early stopping triggered')\n",
        "            break\n",
        "\n",
        "json.dump(history, open(result_dir / 'train_history.json', 'w'), indent=2)\n",
        "print('Best v2 val macro F1:', best_val_f1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_tfCX0SBbJ0G"
      },
      "outputs": [],
      "source": [
        "# v2-fast test evaluation\n",
        "if model_path_v2.exists():\n",
        "    ckpt = torch.load(model_path_v2, map_location=device)\n",
        "    model_v2.load_state_dict(ckpt['model_state'])\n",
        "\n",
        "model_v2.eval()\n",
        "ys, ys_pred = [], []\n",
        "with torch.no_grad():\n",
        "    for xb, yb in test_loader_v2_fast:\n",
        "        xb = xb.to(device)\n",
        "        yb = yb.to(device)\n",
        "        logits = model_v2(xb)\n",
        "        preds = logits.argmax(dim=1)\n",
        "        ys.extend(yb.cpu().numpy().tolist())\n",
        "        ys_pred.extend(preds.cpu().numpy().tolist())\n",
        "\n",
        "test_acc_v2 = accuracy_score(ys, ys_pred)\n",
        "test_f1_v2 = f1_score(ys, ys_pred, average='macro')\n",
        "print(f\"[v2-fast] Test accuracy: {test_acc_v2:.4f}, macro F1: {test_f1_v2:.4f}\")\n",
        "\n",
        "pd.DataFrame({\n",
        "    'path': list(X_test),\n",
        "    'true_label': [idx_to_label[int(i)] for i in ys],\n",
        "    'pred_label': [idx_to_label[int(i)] for i in ys_pred],\n",
        "}).to_csv(save_dir_v2 / 'test_predictions_v2_fast.csv', index=False)\n",
        "\n",
        "print('Saved v2-fast predictions to', save_dir_v2 / 'test_predictions_v2_fast.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oq9EtY8YbJ0I"
      },
      "outputs": [],
      "source": [
        "# Inference helper v2\n",
        "class InferenceModelV2:\n",
        "    def __init__(self, model_path: Path, labels_path: Path):\n",
        "        with open(labels_path, 'r') as f:\n",
        "            self.idx_to_label = {int(k):v for k,v in json.load(f).items()}\n",
        "        ckpt = torch.load(model_path, map_location=device)\n",
        "        cfg = ckpt.get('config', {})\n",
        "        self.img_size = cfg.get('img_size', IMG_SIZE_V2)\n",
        "        self.channels = cfg.get('channels', CHANNELS_V2)\n",
        "        self.mean = cfg.get('mean', [0.5]*self.channels)\n",
        "        self.std = cfg.get('std', [0.5]*self.channels)\n",
        "        # Build model\n",
        "        try:\n",
        "            self.model = build_resnet18_head(len(self.idx_to_label), pretrained=False).to(device)\n",
        "        except Exception:\n",
        "            self.model = model_v2.__class__(len(self.idx_to_label)).to(device)\n",
        "        self.model.load_state_dict(ckpt['model_state'])\n",
        "        self.model.eval()\n",
        "        # Transforms\n",
        "        self.tfms = transforms.Compose([\n",
        "            transforms.Grayscale(num_output_channels=self.channels),\n",
        "            transforms.Resize((self.img_size, self.img_size)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(self.mean, self.std),\n",
        "        ])\n",
        "    @torch.no_grad()\n",
        "    def predict(self, image_path: Path):\n",
        "        with Image.open(image_path) as img:\n",
        "            img = img.convert('L')\n",
        "            x = self.tfms(img).unsqueeze(0).to(device)\n",
        "        logits = self.model(x)\n",
        "        pred = logits.argmax(dim=1).item()\n",
        "        return self.idx_to_label[pred]\n",
        "\n",
        "# Example:\n",
        "# infer_v2 = InferenceModelV2(model_path_v2, labels_path_v2)\n",
        "# infer_v2.predict(Path('Assets/...'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tMvoePoZbJ0J"
      },
      "outputs": [],
      "source": [
        "# (Removed) Baseline inference — use InferenceModelV2 with artifacts_v2\n",
        "'Use InferenceModelV2(model_path_v2, labels_path_v2)'\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}