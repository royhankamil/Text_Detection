{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Handwriting Image Classifier (PyTorch)\n",
        "\n",
        "This notebook trains a CNN to classify handwriting images using the datasets in `Assets/`:\n",
        "- `augmented_images/`: directory-of-directories (each subfolder is a class)\n",
        "- `handwritten-english-characters-and-digits/`: has `train/` and `test/` splits (each has class subfolders)\n",
        "- `image_labels.csv`: has `filename,label` mapping; we'll search for the files within `Assets/`\n",
        "\n",
        "It will:\n",
        "- Discover and merge samples from all three sources\n",
        "- Build stratified train/val/test splits\n",
        "- Train a small CNN on CPU or GPU if available\n",
        "- Evaluate and save the best model and label mapping\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pandas\n",
            "  Downloading pandas-2.3.1-cp313-cp313-win_amd64.whl.metadata (19 kB)\n",
            "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\royha\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2.2.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\royha\\appdata\\roaming\\python\\python313\\site-packages (from pandas) (2.9.0.post0)\n",
            "Collecting pytz>=2020.1 (from pandas)\n",
            "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting tzdata>=2022.7 (from pandas)\n",
            "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\royha\\appdata\\roaming\\python\\python313\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Downloading pandas-2.3.1-cp313-cp313-win_amd64.whl (11.0 MB)\n",
            "   ---------------------------------------- 0.0/11.0 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/11.0 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/11.0 MB ? eta -:--:--\n",
            "   -- ------------------------------------- 0.8/11.0 MB 3.2 MB/s eta 0:00:04\n",
            "   --- ------------------------------------ 1.0/11.0 MB 2.5 MB/s eta 0:00:04\n",
            "   ---- ----------------------------------- 1.3/11.0 MB 2.1 MB/s eta 0:00:05\n",
            "   ------ --------------------------------- 1.8/11.0 MB 2.1 MB/s eta 0:00:05\n",
            "   -------- ------------------------------- 2.4/11.0 MB 2.1 MB/s eta 0:00:05\n",
            "   ----------- ---------------------------- 3.1/11.0 MB 2.4 MB/s eta 0:00:04\n",
            "   -------------- ------------------------- 3.9/11.0 MB 2.6 MB/s eta 0:00:03\n",
            "   ----------------- ---------------------- 4.7/11.0 MB 2.7 MB/s eta 0:00:03\n",
            "   -------------------- ------------------- 5.5/11.0 MB 2.7 MB/s eta 0:00:03\n",
            "   -------------------- ------------------- 5.8/11.0 MB 2.5 MB/s eta 0:00:03\n",
            "   ---------------------- ----------------- 6.3/11.0 MB 2.5 MB/s eta 0:00:02\n",
            "   -------------------------- ------------- 7.3/11.0 MB 2.7 MB/s eta 0:00:02\n",
            "   ----------------------------- ---------- 8.1/11.0 MB 2.8 MB/s eta 0:00:02\n",
            "   ---------------------------------- ----- 9.4/11.0 MB 3.0 MB/s eta 0:00:01\n",
            "   ---------------------------------------  10.7/11.0 MB 3.2 MB/s eta 0:00:01\n",
            "   ---------------------------------------  10.7/11.0 MB 3.2 MB/s eta 0:00:01\n",
            "   ---------------------------------------  10.7/11.0 MB 3.2 MB/s eta 0:00:01\n",
            "   ---------------------------------------  10.7/11.0 MB 3.2 MB/s eta 0:00:01\n",
            "   ---------------------------------------  10.7/11.0 MB 3.2 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 11.0/11.0 MB 2.5 MB/s eta 0:00:00\n",
            "Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
            "Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
            "Installing collected packages: pytz, tzdata, pandas\n",
            "Successfully installed pandas-2.3.1 pytz-2025.2 tzdata-2025.2\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.0.1 -> 25.2\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "pip install pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "# Environment & imports\n",
        "import os, sys, json, random, math, time\n",
        "from pathlib import Path\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "try:\n",
        "    import torchvision\n",
        "    from torchvision import transforms\n",
        "except Exception as e:\n",
        "    print(\"torchvision not found; attempting to continue with PIL-only transforms\")\n",
        "    torchvision = None\n",
        "    transforms = None\n",
        "\n",
        "ASSETS = Path('Assets')\n",
        "assert ASSETS.exists(), f\"Assets folder not found at {ASSETS.resolve()}\"\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "augmented_images: 13640 samples\n",
            "HED train: 2728 samples, HED test: 682 samples\n",
            "CSV samples resolved from image_labels.csv: 0 (missing: 13640)\n",
            "Total unique samples: 17050\n",
            "Num classes: 62\n",
            "Class distribution (top 20): [('0', 275), ('1', 275), ('2', 275), ('3', 275), ('4', 275), ('5', 275), ('6', 275), ('7', 275), ('8', 275), ('9', 275), ('a', 275), ('A_caps', 275), ('b', 275), ('B_caps', 275), ('c', 275), ('C_caps', 275), ('d', 275), ('D_caps', 275), ('e', 275), ('E_caps', 275)]\n"
          ]
        }
      ],
      "source": [
        "# Discover datasets\n",
        "from typing import List, Tuple\n",
        "\n",
        "IMG_EXTS = {'.png', '.jpg', '.jpeg', '.bmp', '.gif'}\n",
        "\n",
        "def list_images_in_dir_of_dirs(root: Path) -> List[Tuple[Path, str]]:\n",
        "    samples = []\n",
        "    if not root.exists():\n",
        "        return samples\n",
        "    for class_dir in sorted([p for p in root.iterdir() if p.is_dir()]):\n",
        "        label = class_dir.name\n",
        "        for p in class_dir.rglob('*'):\n",
        "            if p.suffix.lower() in IMG_EXTS and p.is_file():\n",
        "                samples.append((p, label))\n",
        "    return samples\n",
        "\n",
        "# 1) augmented_images\n",
        "aug_dir = ASSETS / 'augmented_images'\n",
        "aug_samples = list_images_in_dir_of_dirs(aug_dir)\n",
        "print(f\"augmented_images: {len(aug_samples)} samples\")\n",
        "\n",
        "# 2) handwritten-english-characters-and-digits/train and test\n",
        "hed_root = ASSETS / 'handwritten-english-characters-and-digits'\n",
        "hed_train = list_images_in_dir_of_dirs(hed_root / 'train')\n",
        "hed_test = list_images_in_dir_of_dirs(hed_root / 'test')\n",
        "print(f\"HED train: {len(hed_train)} samples, HED test: {len(hed_test)} samples\")\n",
        "\n",
        "# 3) image_labels.csv: filename,label\n",
        "csv_path = ASSETS / 'image_labels.csv'\n",
        "if not csv_path.exists():\n",
        "    alt_csv_path = ASSETS / 'image_label.csv'  # support alternate name\n",
        "    if alt_csv_path.exists():\n",
        "        csv_path = alt_csv_path\n",
        "\n",
        "csv_samples = []\n",
        "if csv_path.exists():\n",
        "    df = pd.read_csv(csv_path)\n",
        "    assert {'filename','label'}.issubset(df.columns)\n",
        "    # Index all images under Assets for filename lookup\n",
        "    all_imgs = {p.name: p for p in ASSETS.rglob('*') if p.suffix.lower() in IMG_EXTS}\n",
        "    missing = 0\n",
        "    for _, row in df.iterrows():\n",
        "        fname = str(row['filename'])\n",
        "        label = str(row['label'])\n",
        "        if fname in all_imgs:\n",
        "            csv_samples.append((all_imgs[fname], label))\n",
        "        else:\n",
        "            # Try to search by suffix match if duplicates are unlikely\n",
        "            matches = [p for n,p in all_imgs.items() if n.endswith(fname)]\n",
        "            if len(matches) == 1:\n",
        "                csv_samples.append((matches[0], label))\n",
        "            else:\n",
        "                missing += 1\n",
        "    print(f\"CSV samples resolved from {csv_path.name}: {len(csv_samples)} (missing: {missing})\")\n",
        "else:\n",
        "    print(\"image_labels.csv not found; skipping CSV source\")\n",
        "\n",
        "# Merge all; if duplicates appear, prefer explicit CSV labels > hed > aug\n",
        "# Use image absolute path as key\n",
        "merged = {}\n",
        "for p,l in aug_samples:\n",
        "    merged[str(p.resolve())] = (p, l)\n",
        "for p,l in hed_train + hed_test:\n",
        "    merged[str(p.resolve())] = (p, l)\n",
        "for p,l in csv_samples:\n",
        "    merged[str(p.resolve())] = (p, l)\n",
        "\n",
        "all_samples = list(merged.values())\n",
        "print(f\"Total unique samples: {len(all_samples)}\")\n",
        "\n",
        "# Map labels to indices\n",
        "labels = sorted(sorted({l for _, l in all_samples}))\n",
        "label_to_idx = {l:i for i,l in enumerate(labels)}\n",
        "idx_to_label = {i:l for l,i in label_to_idx.items()}\n",
        "print(f\"Num classes: {len(labels)}\")\n",
        "\n",
        "# Class distribution\n",
        "counts = Counter([l for _,l in all_samples])\n",
        "print(\"Class distribution (top 20):\", counts.most_common(20))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "13640 1705 1705\n"
          ]
        }
      ],
      "source": [
        "# Train/Val/Test split (stratified)\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "random_seed = 42\n",
        "rng = np.random.RandomState(random_seed)\n",
        "\n",
        "paths = np.array([str(p) for p,_ in all_samples])\n",
        "labels_arr = np.array([label_to_idx[l] for _,l in all_samples])\n",
        "\n",
        "# If HED has an explicit test set, we already included it. We'll still split overall\n",
        "# into train/val/test=0.8/0.1/0.1 stratified.\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(\n",
        "    paths, labels_arr, test_size=0.1, random_state=random_seed, stratify=labels_arr\n",
        ")\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_temp, y_temp, test_size=0.1111, random_state=random_seed, stratify=y_temp\n",
        ")  # 0.8 * 0.1111 ≈ 0.0889 so final ≈ 80/10/10\n",
        "\n",
        "print(len(X_train), len(X_val), len(X_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Transforms and Dataset\n",
        "\n",
        "IMG_SIZE = 64  # can be tuned\n",
        "\n",
        "if transforms is None:\n",
        "    # Minimal transforms using PIL + numpy\n",
        "    class ToTensor:\n",
        "        def __call__(self, img):\n",
        "            arr = np.array(img, dtype=np.float32) / 255.0\n",
        "            if arr.ndim == 2:\n",
        "                arr = arr[..., None]\n",
        "            arr = arr.transpose(2,0,1)\n",
        "            return torch.from_numpy(arr)\n",
        "    basic_train_tfms = None\n",
        "    basic_val_tfms = None\n",
        "else:\n",
        "    basic_train_tfms = transforms.Compose([\n",
        "        transforms.Grayscale(num_output_channels=1),\n",
        "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "        transforms.RandomAffine(degrees=5, translate=(0.02,0.02), scale=(0.95,1.05)),\n",
        "        transforms.RandomPerspective(distortion_scale=0.2, p=0.3),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.5], [0.5]),\n",
        "    ])\n",
        "    basic_val_tfms = transforms.Compose([\n",
        "        transforms.Grayscale(num_output_channels=1),\n",
        "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.5], [0.5]),\n",
        "    ])\n",
        "\n",
        "class HandwritingDataset(Dataset):\n",
        "    def __init__(self, paths: np.ndarray, labels: np.ndarray, transform=None):\n",
        "        self.paths = paths\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "    def __len__(self):\n",
        "        return len(self.paths)\n",
        "    def __getitem__(self, idx):\n",
        "        p = Path(self.paths[idx])\n",
        "        y = int(self.labels[idx])\n",
        "        with Image.open(p) as img:\n",
        "            img = img.convert('L')  # ensure single-channel\n",
        "            if basic_train_tfms is None and transforms is None:\n",
        "                # fallback: resize via PIL then tensor\n",
        "                img = img.resize((IMG_SIZE, IMG_SIZE))\n",
        "                arr = np.array(img, dtype=np.float32)/255.0\n",
        "                arr = arr[None, ...]\n",
        "                x = torch.from_numpy(arr)\n",
        "            else:\n",
        "                x = (self.transform or basic_val_tfms)(img)\n",
        "        return x, y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(13640, 1705, 1705)"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# DataLoaders\n",
        "batch_size = 128\n",
        "num_workers = 2 if os.name != 'nt' else 0  # Windows pytorch dataloader workers\n",
        "\n",
        "train_ds = HandwritingDataset(X_train, y_train, transform=basic_train_tfms)\n",
        "val_ds   = HandwritingDataset(X_val, y_val, transform=basic_val_tfms)\n",
        "test_ds  = HandwritingDataset(X_test, y_test, transform=basic_val_tfms)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
        "val_loader   = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
        "test_loader  = DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
        "\n",
        "len(train_ds), len(val_ds), len(test_ds)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(2206462,\n",
              " SmallCNN(\n",
              "   (features): Sequential(\n",
              "     (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "     (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "     (2): ReLU(inplace=True)\n",
              "     (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "     (4): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "     (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "     (6): ReLU(inplace=True)\n",
              "     (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "     (8): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "     (9): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "     (10): ReLU(inplace=True)\n",
              "     (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "   )\n",
              "   (classifier): Sequential(\n",
              "     (0): Dropout(p=0.3, inplace=False)\n",
              "     (1): Linear(in_features=8192, out_features=256, bias=True)\n",
              "     (2): ReLU(inplace=True)\n",
              "     (3): Dropout(p=0.3, inplace=False)\n",
              "     (4): Linear(in_features=256, out_features=62, bias=True)\n",
              "   )\n",
              " ))"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Model: Small CNN\n",
        "class SmallCNN(nn.Module):\n",
        "    def __init__(self, num_classes: int):\n",
        "        super().__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2),  # 32x32\n",
        "\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2),  # 16x16\n",
        "\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2),  # 8x8\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(128 * (IMG_SIZE//8) * (IMG_SIZE//8), 256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "num_classes = len(labels)\n",
        "model = SmallCNN(num_classes).to(device)\n",
        "\n",
        "sum(p.numel() for p in model.parameters() if p.requires_grad), model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\royha\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 01/15 - loss: 4.1574 - val_acc: 0.0223\n",
            "Saved new best model to artifacts\\handwriting_cnn.pt (val_acc=0.0223)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\royha\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 02/15 - loss: 3.9826 - val_acc: 0.0581\n",
            "Saved new best model to artifacts\\handwriting_cnn.pt (val_acc=0.0581)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\royha\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 03/15 - loss: 3.7718 - val_acc: 0.0933\n",
            "Saved new best model to artifacts\\handwriting_cnn.pt (val_acc=0.0933)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\royha\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 04/15 - loss: 3.5829 - val_acc: 0.1097\n",
            "Saved new best model to artifacts\\handwriting_cnn.pt (val_acc=0.1097)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\royha\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 05/15 - loss: 3.4512 - val_acc: 0.0962\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\royha\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 06/15 - loss: 3.3581 - val_acc: 0.1443\n",
            "Saved new best model to artifacts\\handwriting_cnn.pt (val_acc=0.1443)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\royha\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 07/15 - loss: 3.2953 - val_acc: 0.1830\n",
            "Saved new best model to artifacts\\handwriting_cnn.pt (val_acc=0.1830)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\royha\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 08/15 - loss: 3.2372 - val_acc: 0.2094\n",
            "Saved new best model to artifacts\\handwriting_cnn.pt (val_acc=0.2094)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\royha\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 09/15 - loss: 3.1941 - val_acc: 0.2475\n",
            "Saved new best model to artifacts\\handwriting_cnn.pt (val_acc=0.2475)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\royha\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10/15 - loss: 3.1567 - val_acc: 0.2311\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\royha\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 11/15 - loss: 3.1078 - val_acc: 0.2780\n",
            "Saved new best model to artifacts\\handwriting_cnn.pt (val_acc=0.2780)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\royha\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 12/15 - loss: 3.0871 - val_acc: 0.3202\n",
            "Saved new best model to artifacts\\handwriting_cnn.pt (val_acc=0.3202)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\royha\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 13/15 - loss: 3.0733 - val_acc: 0.2481\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\royha\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 14/15 - loss: 3.0548 - val_acc: 0.2545\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\royha\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 15/15 - loss: 3.0345 - val_acc: 0.3267\n",
            "Saved new best model to artifacts\\handwriting_cnn.pt (val_acc=0.3267)\n",
            "Best val acc: 0.32668621700879763\n"
          ]
        }
      ],
      "source": [
        "# Training utilities\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "def evaluate(model, loader, device):\n",
        "    model.eval()\n",
        "    ys, ys_pred = [], []\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in loader:\n",
        "            xb = xb.to(device)\n",
        "            yb = yb.to(device)\n",
        "            logits = model(xb)\n",
        "            preds = logits.argmax(dim=1)\n",
        "            ys.extend(yb.cpu().numpy().tolist())\n",
        "            ys_pred.extend(preds.cpu().numpy().tolist())\n",
        "    acc = accuracy_score(ys, ys_pred)\n",
        "    return acc, np.array(ys), np.array(ys_pred)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=2, factor=0.5)\n",
        "\n",
        "epochs = 15\n",
        "best_val_acc = 0.0\n",
        "save_dir = Path('artifacts')\n",
        "save_dir.mkdir(exist_ok=True)\n",
        "model_path = save_dir / 'handwriting_cnn.pt'\n",
        "labels_path = save_dir / 'labels.json'\n",
        "\n",
        "with open(labels_path, 'w') as f:\n",
        "    json.dump(idx_to_label, f, indent=2)\n",
        "\n",
        "for epoch in range(1, epochs+1):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    num_batches = 0\n",
        "    for xb, yb in train_loader:\n",
        "        xb = xb.to(device)\n",
        "        yb = yb.to(device)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        logits = model(xb)\n",
        "        loss = criterion(logits, yb)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "        num_batches += 1\n",
        "    train_loss = running_loss / max(1, num_batches)\n",
        "\n",
        "    val_acc, _, _ = evaluate(model, val_loader, device)\n",
        "    scheduler.step(val_acc)\n",
        "\n",
        "    print(f\"Epoch {epoch:02d}/{epochs} - loss: {train_loss:.4f} - val_acc: {val_acc:.4f}\")\n",
        "\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        torch.save({'model_state': model.state_dict(), 'config': {'img_size': IMG_SIZE, 'num_classes': num_classes}}, model_path)\n",
        "        print(f\"Saved new best model to {model_path} (val_acc={best_val_acc:.4f})\")\n",
        "\n",
        "print('Best val acc:', best_val_acc)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy: 0.3273\n",
            "Classification report:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\royha\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
            "c:\\Users\\royha\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
            "c:\\Users\\royha\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        27\n",
            "           1       0.25      0.25      0.25        28\n",
            "           2       0.00      0.00      0.00        28\n",
            "           3       0.36      1.00      0.53        27\n",
            "           4       0.15      0.11      0.13        27\n",
            "           5       0.43      0.44      0.44        27\n",
            "           6       0.43      0.56      0.48        27\n",
            "           7       0.86      0.21      0.34        28\n",
            "           8       0.24      0.56      0.34        27\n",
            "           9       0.22      0.52      0.30        27\n",
            "      A_caps       0.09      0.07      0.08        28\n",
            "      B_caps       0.36      0.61      0.45        28\n",
            "      C_caps       0.45      0.93      0.60        27\n",
            "      D_caps       0.51      0.70      0.59        27\n",
            "      E_caps       0.67      0.29      0.40        28\n",
            "      F_caps       1.00      0.15      0.26        27\n",
            "      G_caps       0.00      0.00      0.00        27\n",
            "      H_caps       0.00      0.00      0.00        27\n",
            "      I_caps       0.24      0.18      0.20        28\n",
            "      J_caps       0.17      0.93      0.29        27\n",
            "      K_caps       1.00      0.04      0.07        28\n",
            "      L_caps       0.76      0.89      0.82        28\n",
            "      M_caps       0.33      0.75      0.46        28\n",
            "      N_caps       0.28      0.71      0.40        28\n",
            "      O_caps       0.50      0.71      0.59        28\n",
            "      P_caps       0.20      0.07      0.11        27\n",
            "      Q_caps       0.42      0.39      0.41        28\n",
            "      R_caps       0.44      0.39      0.42        28\n",
            "      S_caps       0.60      0.11      0.18        28\n",
            "      T_caps       0.80      0.14      0.24        28\n",
            "      U_caps       0.94      0.56      0.70        27\n",
            "      V_caps       0.41      0.89      0.56        27\n",
            "      W_caps       0.43      0.93      0.58        28\n",
            "      X_caps       0.10      0.04      0.05        27\n",
            "      Y_caps       0.33      0.67      0.44        27\n",
            "      Z_caps       0.33      0.50      0.40        28\n",
            "           a       0.41      0.67      0.51        27\n",
            "           b       0.77      0.36      0.49        28\n",
            "           c       0.46      0.46      0.46        28\n",
            "           d       0.00      0.00      0.00        28\n",
            "           e       0.75      0.11      0.19        28\n",
            "           f       0.00      0.00      0.00        27\n",
            "           g       0.00      0.00      0.00        28\n",
            "           h       0.20      0.32      0.24        28\n",
            "           i       0.17      0.46      0.25        28\n",
            "           j       0.00      0.00      0.00        27\n",
            "           k       0.00      0.00      0.00        28\n",
            "           l       0.00      0.00      0.00        27\n",
            "           m       0.25      0.63      0.36        27\n",
            "           n       0.00      0.00      0.00        28\n",
            "           o       0.48      0.59      0.53        27\n",
            "           p       0.00      0.00      0.00        28\n",
            "           q       0.11      0.15      0.13        27\n",
            "           r       0.00      0.00      0.00        27\n",
            "           s       0.50      0.04      0.07        27\n",
            "           t       0.00      0.00      0.00        28\n",
            "           u       0.00      0.00      0.00        27\n",
            "           v       0.00      0.00      0.00        27\n",
            "           w       0.11      0.26      0.16        27\n",
            "           x       1.00      0.37      0.54        27\n",
            "           y       0.21      0.11      0.14        28\n",
            "           z       0.21      0.50      0.29        28\n",
            "\n",
            "    accuracy                           0.33      1705\n",
            "   macro avg       0.32      0.33      0.27      1705\n",
            "weighted avg       0.32      0.33      0.27      1705\n",
            "\n",
            "Confusion matrix shape: (62, 62)\n",
            "Saved predictions to artifacts\\test_predictions.csv\n"
          ]
        }
      ],
      "source": [
        "# Evaluation on test set\n",
        "# Load best model (if saved during training)\n",
        "if model_path.exists():\n",
        "    ckpt = torch.load(model_path, map_location=device)\n",
        "    model.load_state_dict(ckpt['model_state'])\n",
        "\n",
        "test_acc, y_true, y_pred = evaluate(model, test_loader, device)\n",
        "print(f\"Test accuracy: {test_acc:.4f}\")\n",
        "\n",
        "print(\"Classification report:\")\n",
        "print(classification_report(y_true, y_pred, target_names=[idx_to_label[i] for i in range(num_classes)]))\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "print(\"Confusion matrix shape:\", cm.shape)\n",
        "\n",
        "# Save per-image predictions\n",
        "preds_path = save_dir / 'test_predictions.csv'\n",
        "pd.DataFrame({\n",
        "    'path': list(X_test),\n",
        "    'true_label': [idx_to_label[int(i)] for i in y_true],\n",
        "    'pred_label': [idx_to_label[int(i)] for i in y_pred],\n",
        "}).to_csv(preds_path, index=False)\n",
        "print(f\"Saved predictions to {preds_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inference helper\n",
        "class InferenceModel:\n",
        "    def __init__(self, model_path: Path, labels_path: Path):\n",
        "        with open(labels_path, 'r') as f:\n",
        "            self.idx_to_label = {int(k):v for k,v in json.load(f).items()}\n",
        "        ckpt = torch.load(model_path, map_location=device)\n",
        "        cfg = ckpt.get('config', {})\n",
        "        self.img_size = cfg.get('img_size', IMG_SIZE)\n",
        "        num_classes = cfg.get('num_classes', len(self.idx_to_label))\n",
        "        self.model = SmallCNN(num_classes).to(device)\n",
        "        self.model.load_state_dict(ckpt['model_state'])\n",
        "        self.model.eval()\n",
        "        self.tfms = basic_val_tfms\n",
        "    @torch.no_grad()\n",
        "    def predict(self, image_path: Path):\n",
        "        with Image.open(image_path) as img:\n",
        "            img = img.convert('L')\n",
        "            x = self.tfms(img).unsqueeze(0).to(device)\n",
        "        logits = self.model(x)\n",
        "        pred = logits.argmax(dim=1).item()\n",
        "        return self.idx_to_label[pred]\n",
        "\n",
        "# Example (uncomment and set a path)\n",
        "# infer = InferenceModel(model_path, labels_path)\n",
        "# print(infer.predict(Path('Assets/some_image.png')))\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
